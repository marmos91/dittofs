---
phase: 03-storage-management
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go
  - k8s/dittofs-operator/api/v1alpha1/zz_generated.deepcopy.go
  - k8s/dittofs-operator/internal/controller/dittoserver_controller.go
  - k8s/dittofs-operator/pkg/resources/hash.go
autonomous: true

must_haves:
  truths:
    - "User can reference S3 credentials from a Kubernetes Secret"
    - "S3 credentials are injected as environment variables in pod"
    - "Pod restarts when S3 credentials Secret changes"
  artifacts:
    - path: "k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go"
      provides: "S3StoreConfig and S3CredentialsSecretRef types"
      contains: "S3CredentialsSecretRef"
    - path: "k8s/dittofs-operator/internal/controller/dittoserver_controller.go"
      provides: "S3 environment variable injection in container"
      contains: "AWS_ACCESS_KEY_ID"
  key_links:
    - from: "dittoserver_types.go"
      to: "dittoserver_controller.go"
      via: "S3 spec used to inject env vars"
      pattern: "dittoServer\\.Spec\\.S3"
    - from: "buildS3EnvVars function"
      to: "container Env field"
      via: "buildS3EnvVars(dittoServer.Spec.S3) called in container spec"
      pattern: "Env:.*buildS3EnvVars"
    - from: "S3 Secret"
      to: "config hash"
      via: "collectSecretData includes S3 secret"
      pattern: "s3:"
---

<objective>
Add S3 credentials Secret reference support for Cubbit DS3 and AWS S3 payload stores

Purpose: DittoFS supports S3-compatible payload stores (Cubbit DS3, AWS S3). The operator needs to inject S3 credentials into the pod via environment variables. This enables users to configure S3 backend without hardcoding credentials.

Output:
- S3StoreConfig and S3CredentialsSecretRef CRD types
- S3 field in DittoServerSpec
- S3 credentials injected as env vars (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_ENDPOINT_URL)
- S3 Secret included in config hash for pod restart on credential change
</objective>

<execution_context>
@/Users/marmos91/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marmos91/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-storage-management/03-RESEARCH.md
@.planning/phases/03-storage-management/03-01-SUMMARY.md
@k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go
@k8s/dittofs-operator/internal/controller/dittoserver_controller.go
@k8s/dittofs-operator/pkg/resources/hash.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add S3 credential types to CRD</name>
  <files>k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go</files>
  <action>
Add two new types at the end of the types file (before `func init()`):

```go
// S3CredentialsSecretRef references a Secret containing S3-compatible credentials
type S3CredentialsSecretRef struct {
	// Name of the Secret in the same namespace
	// +kubebuilder:validation:Required
	SecretName string `json:"secretName"`

	// Key for access key ID (default: accessKeyId)
	// +kubebuilder:default="accessKeyId"
	AccessKeyIDKey string `json:"accessKeyIdKey,omitempty"`

	// Key for secret access key (default: secretAccessKey)
	// +kubebuilder:default="secretAccessKey"
	SecretAccessKeyKey string `json:"secretAccessKeyKey,omitempty"`

	// Key for S3 endpoint URL (default: endpoint)
	// For Cubbit DS3: https://s3.cubbit.eu
	// +kubebuilder:default="endpoint"
	EndpointKey string `json:"endpointKey,omitempty"`
}

// S3StoreConfig configures S3-compatible payload store credentials
// Note: Actual store creation is done via REST API; this enables
// the operator to inject S3 credentials as environment variables.
type S3StoreConfig struct {
	// CredentialsSecretRef references a Secret with S3 credentials
	// +optional
	CredentialsSecretRef *S3CredentialsSecretRef `json:"credentialsSecretRef,omitempty"`

	// Region for S3 bucket (e.g., "eu-west-1" for Cubbit)
	// +kubebuilder:default="eu-west-1"
	Region string `json:"region,omitempty"`

	// Bucket name (informational; actual config via REST API)
	// +optional
	Bucket string `json:"bucket,omitempty"`
}
```

Then add S3 field to DittoServerSpec (add after SMB field):

```go
// S3 configures S3-compatible payload store credentials
// Credentials are injected as environment variables for the AWS SDK
// +optional
S3 *S3StoreConfig `json:"s3,omitempty"`
```

Run `make generate manifests` to update deepcopy and CRD.
  </action>
  <verify>
Run:
```bash
cd k8s/dittofs-operator && make generate manifests
grep "S3CredentialsSecretRef" api/v1alpha1/dittoserver_types.go
grep "S3StoreConfig" api/v1alpha1/dittoserver_types.go
grep -A20 "s3:" config/crd/bases/dittofs.dittofs.com_dittoservers.yaml
```
All types should exist, CRD should have s3 section.
  </verify>
  <done>S3CredentialsSecretRef and S3StoreConfig types exist, S3 field in DittoServerSpec, CRD manifest updated</done>
</task>

<task type="auto">
  <name>Task 2: Inject S3 credentials as environment variables</name>
  <files>k8s/dittofs-operator/internal/controller/dittoserver_controller.go</files>
  <action>
In reconcileStatefulSet function, modify the container spec to include S3 environment variables:

1. Create a helper function `buildS3EnvVars` near the bottom of the file:

```go
// buildS3EnvVars creates environment variables for S3 credentials from Secret reference.
// Returns nil if S3 is not configured.
func buildS3EnvVars(spec *dittoiov1alpha1.S3StoreConfig) []corev1.EnvVar {
	if spec == nil || spec.CredentialsSecretRef == nil {
		return nil
	}

	ref := spec.CredentialsSecretRef

	// Apply defaults for key names
	accessKeyIDKey := ref.AccessKeyIDKey
	if accessKeyIDKey == "" {
		accessKeyIDKey = "accessKeyId"
	}
	secretAccessKeyKey := ref.SecretAccessKeyKey
	if secretAccessKeyKey == "" {
		secretAccessKeyKey = "secretAccessKey"
	}
	endpointKey := ref.EndpointKey
	if endpointKey == "" {
		endpointKey = "endpoint"
	}

	envVars := []corev1.EnvVar{
		{
			Name: "AWS_ACCESS_KEY_ID",
			ValueFrom: &corev1.EnvVarSource{
				SecretKeyRef: &corev1.SecretKeySelector{
					LocalObjectReference: corev1.LocalObjectReference{
						Name: ref.SecretName,
					},
					Key: accessKeyIDKey,
				},
			},
		},
		{
			Name: "AWS_SECRET_ACCESS_KEY",
			ValueFrom: &corev1.EnvVarSource{
				SecretKeyRef: &corev1.SecretKeySelector{
					LocalObjectReference: corev1.LocalObjectReference{
						Name: ref.SecretName,
					},
					Key: secretAccessKeyKey,
				},
			},
		},
		{
			Name: "AWS_ENDPOINT_URL",
			ValueFrom: &corev1.EnvVarSource{
				SecretKeyRef: &corev1.SecretKeySelector{
					LocalObjectReference: corev1.LocalObjectReference{
						Name: ref.SecretName,
					},
					Key:      endpointKey,
					Optional: boolPtr(true), // Endpoint is optional (AWS doesn't need it)
				},
			},
		},
	}

	// Add region if specified
	if spec.Region != "" {
		envVars = append(envVars, corev1.EnvVar{
			Name:  "AWS_REGION",
			Value: spec.Region,
		})
	}

	return envVars
}

// boolPtr returns a pointer to a bool value.
func boolPtr(b bool) *bool {
	return &b
}
```

2. In reconcileStatefulSet, add the env vars to the container spec. Find the Containers slice and add Env field:

```go
Containers: []corev1.Container{
    {
        Name:            "dittofs",
        Image:           dittoServer.Spec.Image,
        Command:         []string{"/app/dittofs"},
        Args:            []string{"start", "--config", "/config/config.yaml"},
        VolumeMounts:    volumeMounts,
        Resources:       dittoServer.Spec.Resources,
        SecurityContext: dittoServer.Spec.SecurityContext,
        Ports:           buildContainerPorts(dittoServer),
        Env:             buildS3EnvVars(dittoServer.Spec.S3), // ADD THIS LINE
        // ... rest of probes
    },
},
```
  </action>
  <verify>
Run:
```bash
cd k8s/dittofs-operator && go build ./...
grep -A30 "func buildS3EnvVars" internal/controller/dittoserver_controller.go
grep "AWS_ACCESS_KEY_ID" internal/controller/dittoserver_controller.go
grep "Env:.*buildS3EnvVars" internal/controller/dittoserver_controller.go
```
Build should succeed, buildS3EnvVars function should exist with AWS env vars, and Env field should call buildS3EnvVars(dittoServer.Spec.S3).
  </verify>
  <done>S3 credentials injected as AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_ENDPOINT_URL, AWS_REGION env vars; buildS3EnvVars wired to container spec</done>
</task>

<task type="auto">
  <name>Task 3: Include S3 Secret in config hash</name>
  <files>k8s/dittofs-operator/internal/controller/dittoserver_controller.go</files>
  <action>
Modify the `collectSecretData` function to include S3 credentials Secret:

Add this block at the end of the function (before the return statement):

```go
// S3 credentials secret (if configured)
if dittoServer.Spec.S3 != nil && dittoServer.Spec.S3.CredentialsSecretRef != nil {
    secret := &corev1.Secret{}
    if err := r.Get(ctx, client.ObjectKey{
        Namespace: dittoServer.Namespace,
        Name:      dittoServer.Spec.S3.CredentialsSecretRef.SecretName,
    }, secret); err != nil {
        return nil, fmt.Errorf("failed to get S3 credentials secret: %w", err)
    }

    // Include all data from the secret for hash
    for k, v := range secret.Data {
        secrets["s3:"+k] = v
    }
}
```

This ensures that when S3 credentials change, the config hash changes, triggering a pod restart.
  </action>
  <verify>
Run:
```bash
cd k8s/dittofs-operator && go build ./...
grep -B5 -A15 "S3 credentials secret" internal/controller/dittoserver_controller.go
```
Build should succeed, S3 secret collection should be present in collectSecretData.
  </verify>
  <done>S3 Secret included in config hash computation, pod restarts when S3 credentials change</done>
</task>

<task type="auto">
  <name>Task 4: Create S3 credentials sample Secret and update tests</name>
  <files>k8s/dittofs-operator/config/samples/dittofs_s3_secret.yaml</files>
  <action>
1. Create a sample S3 credentials Secret at `k8s/dittofs-operator/config/samples/dittofs_s3_secret.yaml`:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: dittofs-s3-credentials
  namespace: dittofs-system
type: Opaque
stringData:
  accessKeyId: "your-access-key-id"
  secretAccessKey: "your-secret-access-key"
  endpoint: "https://s3.cubbit.eu"  # For Cubbit DS3, or omit for AWS S3
```

2. Create a sample CR that uses S3 at `k8s/dittofs-operator/config/samples/dittofs_v1alpha1_dittofs_s3.yaml`:

```yaml
apiVersion: dittofs.dittofs.com/v1alpha1
kind: DittoServer
metadata:
  name: dittofs-s3-sample
  namespace: dittofs-system
spec:
  image: marmos91c/dittofs:latest
  replicas: 1
  storage:
    metadataSize: "10Gi"
    cacheSize: "5Gi"
    # No contentSize needed - using S3 for payload
  s3:
    credentialsSecretRef:
      secretName: dittofs-s3-credentials
      # Using defaults for keys: accessKeyId, secretAccessKey, endpoint
    region: "eu-west-1"
    bucket: "dittofs-data"
  identity:
    jwt:
      secretRef:
        name: dittofs-jwt-secret
        key: secret
    admin:
      username: admin
      passwordSecretRef:
        name: dittofs-admin-secret
        key: password-hash
```

3. Run tests to ensure nothing is broken:
```bash
cd k8s/dittofs-operator && make test
```
  </action>
  <verify>
Run:
```bash
cd k8s/dittofs-operator && make test
ls k8s/dittofs-operator/config/samples/dittofs_s3_secret.yaml
ls k8s/dittofs-operator/config/samples/dittofs_v1alpha1_dittofs_s3.yaml
grep "credentialsSecretRef" k8s/dittofs-operator/config/samples/dittofs_v1alpha1_dittofs_s3.yaml
```
Tests should pass, sample files should exist.
  </verify>
  <done>S3 credentials sample Secret and S3-configured DittoServer sample CR exist, tests pass</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **CRD schema check**:
```bash
cd k8s/dittofs-operator
grep -A30 "s3:" config/crd/bases/dittofs.dittofs.com_dittoservers.yaml
```
Should show s3 section with credentialsSecretRef, region, bucket fields.

2. **Build check**:
```bash
cd k8s/dittofs-operator && go build ./...
```
Must succeed.

3. **Test check**:
```bash
cd k8s/dittofs-operator && make test
```
All tests must pass.

4. **Env var injection**:
```bash
grep "AWS_ACCESS_KEY_ID\|AWS_SECRET_ACCESS_KEY\|AWS_ENDPOINT_URL\|AWS_REGION" k8s/dittofs-operator/internal/controller/dittoserver_controller.go
```
All four env vars should be present.

5. **Wiring check**:
```bash
grep "Env:.*buildS3EnvVars" k8s/dittofs-operator/internal/controller/dittoserver_controller.go
```
Should find the wiring of buildS3EnvVars to container Env field.
</verification>

<success_criteria>
- S3CredentialsSecretRef and S3StoreConfig types exist in CRD
- S3 credentials injected as AWS SDK-compatible environment variables
- buildS3EnvVars function is called in container spec (Env: buildS3EnvVars(...))
- S3 Secret included in config hash (pod restarts on credential change)
- Sample S3 Secret and S3-configured DittoServer CR exist
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-storage-management/03-02-SUMMARY.md` using summary template.
</output>
