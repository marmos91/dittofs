---
phase: 01-locking-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - pkg/metadata/lock_persistence.go
  - pkg/metadata/lock_persistence_test.go
  - pkg/metadata/store/store.go
  - pkg/metadata/store/memory/locks.go
  - pkg/metadata/store/memory/locks_test.go
  - pkg/metadata/store/badger/locks.go
  - pkg/metadata/store/badger/locks_test.go
  - pkg/metadata/store/postgres/locks.go
  - pkg/metadata/store/postgres/locks_test.go
  - pkg/metadata/store/postgres/migrations/004_locks.go
autonomous: true

must_haves:
  truths:
    - "Lock state persists across server restart (BadgerDB/PostgreSQL)"
    - "Lock operations are atomic with metadata store transactions"
    - "All three store implementations (memory, badger, postgres) support lock persistence"
    - "Locks can be queried by file, owner, or client"
  artifacts:
    - path: "pkg/metadata/lock_persistence.go"
      provides: "LockStore interface and PersistedLock type"
      exports: ["LockStore", "PersistedLock", "LockQuery"]
    - path: "pkg/metadata/store/memory/locks.go"
      provides: "In-memory LockStore implementation"
      contains: "func.*LockStore"
    - path: "pkg/metadata/store/badger/locks.go"
      provides: "BadgerDB LockStore implementation"
      contains: "func.*LockStore"
    - path: "pkg/metadata/store/postgres/locks.go"
      provides: "PostgreSQL LockStore implementation"
      contains: "func.*LockStore"
  key_links:
    - from: "pkg/metadata/store/store.go"
      to: "pkg/metadata/lock_persistence.go"
      via: "LockStore embedded in Transaction interface"
      pattern: "LockStore"
    - from: "pkg/metadata/store/badger/locks.go"
      to: "pkg/metadata/lock_persistence.go"
      via: "implements LockStore interface"
      pattern: "type.*LockStore"
---

<objective>
Implement lock persistence across all metadata store backends (memory, BadgerDB, PostgreSQL).

Purpose: Locks must survive server restart for production use. This plan adds the LockStore interface and implementations that persist EnhancedLock to the metadata store using the existing transaction abstraction.

Output:
- LockStore interface for lock CRUD operations
- Memory implementation (for testing, ephemeral)
- BadgerDB implementation (persistent, embedded)
- PostgreSQL implementation (persistent, distributed)
</objective>

<execution_context>
@/Users/marmos91/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marmos91/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-locking-infrastructure/01-CONTEXT.md
@.planning/phases/01-locking-infrastructure/01-RESEARCH.md

# Plan 01 creates the lock types we persist
@.planning/phases/01-locking-infrastructure/01-01-SUMMARY.md

# Existing store patterns to follow
@pkg/metadata/store.go
@pkg/metadata/store/memory/store.go
@pkg/metadata/store/badger/store.go
@pkg/metadata/store/badger/encoding.go
@pkg/metadata/store/postgres/store.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: LockStore interface and memory implementation</name>
  <files>
    pkg/metadata/lock_persistence.go
    pkg/metadata/lock_persistence_test.go
    pkg/metadata/store/store.go
    pkg/metadata/store/memory/locks.go
    pkg/metadata/store/memory/locks_test.go
  </files>
  <action>
Create pkg/metadata/lock_persistence.go defining the LockStore interface:

1. Define `PersistedLock` struct (serializable form of EnhancedLock):
   - `ID string` - Lock UUID
   - `ShareName string` - Share this lock belongs to
   - `FileID string` - File identifier (from FileHandle)
   - `OwnerID string` - Protocol-provided owner identifier
   - `ClientID string` - Connection tracker client ID
   - `LockType int` - 0=shared, 1=exclusive
   - `Offset uint64` - Starting byte offset
   - `Length uint64` - Bytes locked (0 = to EOF)
   - `ShareReservation int` - SMB share mode
   - `AcquiredAt time.Time` - When lock was acquired
   - `ServerEpoch uint64` - For split-brain detection

2. Define `LockQuery` struct for filtering:
   - `FileHandle FileHandle` - Filter by file (optional)
   - `OwnerID string` - Filter by owner (optional)
   - `ClientID string` - Filter by client (optional)
   - `ShareName string` - Filter by share (optional)

3. Define `LockStore` interface:
```go
type LockStore interface {
    // PutLock persists a lock. Overwrites if lock with same ID exists.
    PutLock(ctx context.Context, lock *PersistedLock) error

    // GetLock retrieves a lock by ID.
    GetLock(ctx context.Context, lockID string) (*PersistedLock, error)

    // DeleteLock removes a lock by ID.
    DeleteLock(ctx context.Context, lockID string) error

    // ListLocks returns locks matching the query.
    // Empty query returns all locks.
    ListLocks(ctx context.Context, query LockQuery) ([]*PersistedLock, error)

    // DeleteLocksByClient removes all locks for a client.
    // Returns number of locks deleted.
    DeleteLocksByClient(ctx context.Context, clientID string) (int, error)

    // DeleteLocksByFile removes all locks for a file.
    // Returns number of locks deleted.
    DeleteLocksByFile(ctx context.Context, fileHandle FileHandle) (int, error)

    // GetServerEpoch returns current server epoch.
    GetServerEpoch(ctx context.Context) (uint64, error)

    // IncrementServerEpoch increments and returns new epoch.
    IncrementServerEpoch(ctx context.Context) (uint64, error)
}
```

4. Add conversion functions:
   - `ToPersistedLock(lock *EnhancedLock, shareName string, epoch uint64) *PersistedLock`
   - `FromPersistedLock(pl *PersistedLock) *EnhancedLock`

5. Update pkg/metadata/store/store.go:
   - Add `LockStore` to the `Transaction` interface (embedded)
   - This enables lock operations within metadata transactions

6. Create pkg/metadata/store/memory/locks.go:
   - Implement LockStore for memory store
   - Use map[string]*PersistedLock for storage
   - Implement all query patterns using linear search (acceptable for memory)
   - Store epoch as simple uint64 field

7. Create pkg/metadata/store/memory/locks_test.go:
   - Test all CRUD operations
   - Test query filtering (by file, owner, client)
   - Test DeleteLocksByClient removes correct locks
   - Test epoch increment
  </action>
  <verify>
```bash
go test -v ./pkg/metadata/... -run "TestLockStore|TestPersistedLock"
go test -v ./pkg/metadata/store/memory/... -run "TestLock"
go build ./pkg/metadata/...
```
  </verify>
  <done>
- LockStore interface defined with all methods
- PersistedLock and LockQuery types defined
- Conversion functions work correctly
- Transaction interface includes LockStore
- Memory implementation passes all tests
  </done>
</task>

<task type="auto">
  <name>Task 2: BadgerDB lock persistence implementation</name>
  <files>
    pkg/metadata/store/badger/locks.go
    pkg/metadata/store/badger/locks_test.go
    pkg/metadata/store/badger/encoding.go
  </files>
  <action>
Create pkg/metadata/store/badger/locks.go implementing LockStore for BadgerDB:

1. Define key prefixes in encoding.go (follow existing pattern):
   - `prefixLock = "lock:"` - Primary: lock:{lockID}
   - `prefixLockByFile = "lkfile:"` - Index: lkfile:{fileID}:{lockID}
   - `prefixLockByOwner = "lkowner:"` - Index: lkowner:{ownerID}:{lockID}
   - `prefixLockByClient = "lkclient:"` - Index: lkclient:{clientID}:{lockID}
   - `prefixServerEpoch = "srvepoch"` - Single key for epoch

2. Implement PutLock:
   - JSON encode PersistedLock
   - Store primary key: lock:{lockID}
   - Store secondary indexes for efficient queries
   - Use BadgerDB transaction for atomicity

3. Implement GetLock:
   - Read from lock:{lockID}
   - JSON decode to PersistedLock

4. Implement DeleteLock:
   - Delete primary key and all secondary indexes
   - Must read lock first to know which indexes to delete

5. Implement ListLocks with query:
   - If FileHandle set: scan lkfile:{fileID}: prefix
   - If OwnerID set: scan lkowner:{ownerID}: prefix
   - If ClientID set: scan lkclient:{clientID}: prefix
   - If ShareName set: filter results by ShareName field
   - Empty query: scan lock: prefix (all locks)
   - Return PersistedLock array

6. Implement DeleteLocksByClient:
   - Scan lkclient:{clientID}: prefix
   - Delete each lock and its indexes
   - Return count

7. Implement DeleteLocksByFile:
   - Scan lkfile:{fileID}: prefix
   - Delete each lock and its indexes
   - Return count

8. Implement GetServerEpoch:
   - Read srvepoch key
   - Return 0 if not found (fresh start)

9. Implement IncrementServerEpoch:
   - Read current epoch
   - Increment and write back
   - Return new value
   - Use transaction for atomicity

Create pkg/metadata/store/badger/locks_test.go:
- Integration tests with real BadgerDB (in-memory mode)
- Test persistence: write, close, reopen, verify locks exist
- Test secondary index correctness
- Test DeleteLocksByClient removes all client locks
- Test concurrent access
- Test epoch persistence and increment
  </action>
  <verify>
```bash
go test -v ./pkg/metadata/store/badger/... -run "TestLock"
go test -race ./pkg/metadata/store/badger/... -run "TestLock"
```
  </verify>
  <done>
- BadgerDB LockStore implementation complete
- Secondary indexes enable efficient queries
- Locks persist across store close/reopen
- DeleteLocksByClient/File correctly maintains indexes
- Epoch persists and increments correctly
- All tests pass including race detector
  </done>
</task>

<task type="auto">
  <name>Task 3: PostgreSQL lock persistence implementation</name>
  <files>
    pkg/metadata/store/postgres/locks.go
    pkg/metadata/store/postgres/locks_test.go
    pkg/metadata/store/postgres/migrations/004_locks.go
  </files>
  <action>
Create PostgreSQL migration pkg/metadata/store/postgres/migrations/004_locks.go:

```sql
CREATE TABLE locks (
    id              UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    share_name      TEXT NOT NULL,
    file_id         TEXT NOT NULL,
    owner_id        TEXT NOT NULL,
    client_id       TEXT NOT NULL,
    lock_type       SMALLINT NOT NULL,
    byte_offset     BIGINT NOT NULL,
    byte_length     BIGINT NOT NULL,
    share_reservation SMALLINT NOT NULL DEFAULT 0,
    acquired_at     TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    server_epoch    BIGINT NOT NULL,

    CONSTRAINT valid_lock_type CHECK (lock_type IN (0, 1)),
    CONSTRAINT valid_offset CHECK (byte_offset >= 0),
    CONSTRAINT valid_length CHECK (byte_length >= 0),
    CONSTRAINT valid_share_reservation CHECK (share_reservation >= 0 AND share_reservation <= 3)
);

CREATE INDEX idx_locks_file_id ON locks(file_id);
CREATE INDEX idx_locks_owner_id ON locks(owner_id);
CREATE INDEX idx_locks_client_id ON locks(client_id);
CREATE INDEX idx_locks_share_name ON locks(share_name);

CREATE TABLE server_epoch (
    id          INTEGER PRIMARY KEY DEFAULT 1,
    epoch       BIGINT NOT NULL DEFAULT 0,
    updated_at  TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT single_row CHECK (id = 1)
);

INSERT INTO server_epoch (id, epoch) VALUES (1, 0);
```

Create pkg/metadata/store/postgres/locks.go implementing LockStore:

1. Implement PutLock:
   - Use INSERT ... ON CONFLICT (id) DO UPDATE (upsert pattern)
   - Map PersistedLock fields to columns

2. Implement GetLock:
   - SELECT by id
   - Return ErrLockNotFound if no rows

3. Implement DeleteLock:
   - DELETE by id
   - Return ErrLockNotFound if no rows affected

4. Implement ListLocks:
   - Build dynamic WHERE clause from LockQuery
   - Use parameterized queries to prevent SQL injection
   - Return empty slice if no matches

5. Implement DeleteLocksByClient:
   - DELETE WHERE client_id = $1
   - Return rows affected count

6. Implement DeleteLocksByFile:
   - DELETE WHERE file_id = $1
   - Return rows affected count

7. Implement GetServerEpoch:
   - SELECT epoch FROM server_epoch WHERE id = 1
   - Single row table pattern

8. Implement IncrementServerEpoch:
   - UPDATE server_epoch SET epoch = epoch + 1, updated_at = NOW() WHERE id = 1 RETURNING epoch
   - Use RETURNING to get new value atomically

Create pkg/metadata/store/postgres/locks_test.go:
- Skip if no PostgreSQL test connection available
- Test all CRUD operations
- Test query filtering
- Test epoch operations
- Use transactions and rollback for test isolation
  </action>
  <verify>
```bash
# Unit tests (may skip if no DB)
go test -v ./pkg/metadata/store/postgres/... -run "TestLock"

# Verify migration compiles
go build ./pkg/metadata/store/postgres/...
```
  </verify>
  <done>
- PostgreSQL migration creates locks and server_epoch tables
- PostgreSQL LockStore implementation complete
- Indexes optimize query patterns
- Atomic epoch increment using RETURNING
- Tests pass (or skip gracefully without DB)
  </done>
</task>

</tasks>

<verification>
```bash
# Run all store tests
go test -v ./pkg/metadata/store/...

# Run with race detector
go test -race ./pkg/metadata/store/...

# Verify build
go build ./...

# Check for lint issues
go vet ./pkg/metadata/...
```
</verification>

<success_criteria>
1. LockStore interface defined and documented
2. Memory implementation works for testing
3. BadgerDB implementation persists locks with secondary indexes
4. PostgreSQL implementation uses proper schema with indexes
5. All implementations pass the same test patterns
6. Server epoch tracks restarts for stale lock detection
7. Lock operations can be used within metadata transactions
</success_criteria>

<output>
After completion, create `.planning/phases/01-locking-infrastructure/01-02-SUMMARY.md`
</output>
