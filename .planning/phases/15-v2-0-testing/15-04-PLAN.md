---
phase: 15-v2-0-testing
plan: 04
type: execute
wave: 2
depends_on: ["15-01"]
files_modified:
  - test/e2e/nfsv4_store_matrix_test.go
  - test/e2e/nfsv4_recovery_test.go
autonomous: true
requirements:
  - TEST2-01
  - TEST2-06

must_haves:
  truths:
    - "NFSv4 file I/O works across all 9 storage backend combinations"
    - "NFSv3 file I/O still works across all 9 storage backend combinations"
    - "Two shares mounted simultaneously via NFSv4 both function correctly"
    - "Server restart with persistent metadata preserves file data"
    - "After server restart with memory backend, previously-created files return ENOENT (new empty state)"
    - "File size matrix (500KB, 1MB, 10MB, 100MB) passes for both v3 and v4"
  artifacts:
    - path: "test/e2e/nfsv4_store_matrix_test.go"
      provides: "Version-parameterized store matrix E2E tests"
      contains: "TestStoreMatrixV4"
    - path: "test/e2e/nfsv4_recovery_test.go"
      provides: "Server restart and recovery E2E tests"
      contains: "TestServerRestartRecovery"
  key_links:
    - from: "test/e2e/nfsv4_store_matrix_test.go"
      to: "test/e2e/framework/mount.go"
      via: "MountNFSExportWithVersion"
      pattern: "framework\\.MountNFS(Export)?WithVersion"
    - from: "test/e2e/nfsv4_store_matrix_test.go"
      to: "test/e2e/framework/containers.go"
      via: "PostgresHelper, LocalstackHelper for backend containers"
      pattern: "framework\\.(NewPostgresHelper|NewLocalstackHelper)"
---

<objective>
Implement version-parameterized store matrix E2E tests (v3+v4 x all backends), multi-share concurrent mounts, file size matrix, and server restart/recovery tests.

Purpose: Validates that NFSv4 works correctly across all storage backend combinations (not just memory/memory) and that server lifecycle events (restart, recovery) are handled correctly. Also confirms NFSv3 backward compatibility across the full matrix.

Output: nfsv4_store_matrix_test.go with full version x backend matrix, nfsv4_recovery_test.go with server restart and recovery tests.
</objective>

<execution_context>
@/Users/marmos91/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marmos91/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-v2-0-testing/15-CONTEXT.md
@.planning/phases/15-v2-0-testing/15-RESEARCH.md
@.planning/phases/15-v2-0-testing/15-01-SUMMARY.md
@test/e2e/store_matrix_test.go
@test/e2e/framework/containers.go
@test/e2e/helpers/stores.go
@test/e2e/helpers/server.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Version-parameterized store matrix and multi-share tests</name>
  <files>test/e2e/nfsv4_store_matrix_test.go</files>
  <action>
Create test/e2e/nfsv4_store_matrix_test.go with `//go:build e2e` tag, package `e2e`.

This extends the existing store_matrix_test.go pattern to run against BOTH NFSv3 and NFSv4.0 mounts. The existing store_matrix_test.go tests v3 only -- this new file tests the version dimension.

Implement:

1. `TestStoreMatrixV4(t *testing.T)` -- full version x backend matrix:
   - Define version matrix: ["3", "4.0"]
   - Define store matrix (reuse storeConfig from store_matrix_test.go or define locally): all 9 combos of metadata (memory/badger/postgres) x payload (memory/filesystem/s3)
   - Check container availability (Postgres, Localstack) once at the start
   - For each version x store combination (18 total subtests):
     - `t.Run(fmt.Sprintf("%s/%s/%s", version, meta, payload), func(t *testing.T) { ... })`
     - Skip if containers unavailable
     - Start server process
     - Create stores, share, enable NFS adapter
     - Mount with MountNFSExportWithVersion (or MountNFSWithVersion depending on Plan 01's API)
     - Run basic file ops: create/read/write, directory ops, delete
     - On v4.0: skip on macOS via SkipIfNFSv4Unsupported

   Per CONTEXT.md: "full cross-product for I/O tests (read/write/create)". This test satisfies that requirement.

2. `TestFileSizeMatrix(t *testing.T)` -- file size tests for both versions:
   - Sizes: 500KB, 1MB, 10MB, 100MB (per CONTEXT.md decision)
   - For each version in [3, 4.0]:
     - Start server, create memory/memory share, enable NFS, mount
     - For each size:
       - Write random file of that size
       - Read back and verify checksum
       - Delete file
   - 100MB test gated behind `if !testing.Short()` to avoid CI slowness

3. `TestMultiShareConcurrent(t *testing.T)` -- two shares mounted simultaneously:
   - Start server
   - Create two shares: /share-alpha (memory/memory), /share-beta (memory/memory)
   - Enable NFS adapter
   - For each version in [3, 4.0]:
     - Mount both shares simultaneously
     - Write file to /share-alpha, write file to /share-beta
     - Verify files are isolated (file in alpha not visible in beta)
     - Verify both files readable
     - Unmount both
   - Tests concurrent multi-share access per CONTEXT.md decision

4. `TestMultiClientConcurrency(t *testing.T)` -- two mounts to same share:
   - Start server, create share, enable NFS
   - For each version in [3, 4.0]:
     - Mount same share twice (different mount points)
     - Mount1: write file "from_mount1.txt"
     - Mount2: write file "from_mount2.txt"
     - Verify mount1 can read "from_mount2.txt" and vice versa
     - Concurrent write test: both mounts write to different files simultaneously (goroutines), verify no corruption via checksums
  </action>
  <verify>
`go build -tags=e2e ./test/e2e/...` compiles.
`go vet -tags=e2e ./test/e2e/...` passes.
  </verify>
  <done>
nfsv4_store_matrix_test.go contains: TestStoreMatrixV4 (18 subtests: 2 versions x 9 backends), TestFileSizeMatrix (v3+v4 x 4 sizes), TestMultiShareConcurrent (v3+v4 x 2 shares), TestMultiClientConcurrency (v3+v4 x concurrent access). All compile with -tags=e2e.
  </done>
</task>

<task type="auto">
  <name>Task 2: Server restart/recovery and squash behavior tests</name>
  <files>test/e2e/nfsv4_recovery_test.go</files>
  <action>
Create test/e2e/nfsv4_recovery_test.go with `//go:build e2e` tag, package `e2e`.

Implement:

1. `TestServerRestartRecovery(t *testing.T)`:
   - Start server with BadgerDB metadata + filesystem payload (persistent backends)
   - Create share, enable NFS, mount v3 and v4
   - Write files via both mounts
   - Unmount both
   - Stop server gracefully (SIGTERM, wait for exit)
   - Start NEW server process with SAME data directory paths
   - Re-enable NFS adapter (same port)
   - Re-mount v3 and v4
   - Verify files written before restart are still accessible (data persisted)
   - Write new files after restart, verify accessible
   - Per CONTEXT.md: "server restart/recovery test (graceful shutdown + restart + state reclaim)"

2. `TestStaleNFSHandle(t *testing.T)`:
   - Start server with memory backend (ephemeral -- state lost on restart)
   - Mount v4, create file, read file (confirm accessible)
   - Unmount
   - Stop server (ForceKill)
   - Start NEW server (new state, memory backend = empty)
   - Re-mount v4
   - Try to access previously-created filename -- expect ENOENT (file doesn't exist in new state, since memory backend was wiped)
   - NOTE: The behavior is ENOENT (not ESTALE) because the new server has a completely new empty filesystem -- the file simply does not exist. ESTALE would occur if the client had a cached file handle from the old server and tried to use it without re-mounting, but since we unmount and re-mount, the client does a fresh LOOKUP which returns ENOENT.
   - Per CONTEXT.md: "Test stale NFS file handles (access after server restart with memory backend)"

3. `TestSquashBehavior(t *testing.T)`:
   - For each version in [3, 4.0]:
     - Test root_squash: mount as root, create file, verify file owned by nobody (uid mapped to squash uid)
     - Test all_squash: mount, create file, verify all files owned by squash uid/gid
     - NOTE: squash behavior depends on server-side share configuration. Configure share with appropriate squash settings via API before mounting.
   - Per CONTEXT.md: "Test squash behavior (root_squash, all_squash) at mount level"

4. `TestClientReconnection(t *testing.T)`:
   - Start server, mount v4, write file
   - Briefly stop NFS adapter via API (disable adapter)
   - Re-enable NFS adapter (same port)
   - Try to read file again -- NFSv4 client should reconnect transparently
   - Per CONTEXT.md: "Include client reconnection test after brief network disruption"
   - Use generous timeout (30s) for client reconnection

For persistent backend tests: use t.TempDir() for BadgerDB path and filesystem payload path. These directories persist across server restart within the same test.
  </action>
  <verify>
`go build -tags=e2e ./test/e2e/...` compiles.
`go vet -tags=e2e ./test/e2e/...` passes.
  </verify>
  <done>
nfsv4_recovery_test.go contains: TestServerRestartRecovery (persistent backend restart), TestStaleNFSHandle (memory backend restart -> ENOENT because new empty state after re-mount), TestSquashBehavior (root_squash/all_squash for v3+v4), TestClientReconnection (adapter disable/re-enable). All compile with -tags=e2e.
  </done>
</task>

</tasks>

<verification>
- `go build -tags=e2e ./test/e2e/...` compiles successfully
- `go vet -tags=e2e ./test/e2e/...` passes
- Store matrix tests cover all 18 version x backend combinations
- File size matrix tests 500KB/1MB/10MB/100MB for both v3 and v4
- Multi-share and multi-client tests verify isolation and concurrent access
- Recovery tests verify server restart with persistent backends preserves data
- Stale handle test verifies ENOENT after memory backend restart (re-mount scenario)
</verification>

<success_criteria>
- Full version x backend store matrix validated (v3+v4 x all 9 combinations)
- File sizes from 500KB to 100MB tested for both protocol versions
- Multi-share concurrent mounts verified
- Server restart/recovery with persistent backends confirmed
- NFSv3 backward compatibility fully confirmed through store matrix
</success_criteria>

<output>
After completion, create `.planning/phases/15-v2-0-testing/15-04-SUMMARY.md`
</output>
