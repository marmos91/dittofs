---
phase: 03-nsm-protocol
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - internal/protocol/nsm/callback/client.go
  - internal/protocol/nsm/callback/notify.go
  - internal/protocol/nsm/notifier.go
  - internal/protocol/nlm/handlers/free_all.go
  - internal/protocol/nlm/handlers/context.go
  - internal/protocol/nlm/types/constants.go
  - internal/protocol/nlm/dispatch.go
  - internal/protocol/nsm/metrics.go
  - pkg/adapter/nfs/nfs_adapter.go
autonomous: true

must_haves:
  truths:
    - "SM_NOTIFY callbacks sent with 5s total timeout"
    - "Server restart triggers parallel SM_NOTIFY to all registered clients"
    - "Failed SM_NOTIFY triggers lock cleanup for that client"
    - "FREE_ALL (NLM procedure 23) releases all locks for a client"
    - "Waiting lock requests proceed after crashed client locks released"
    - "NSM metrics track registrations, notifications, and crashes"
  artifacts:
    - path: "internal/protocol/nsm/callback/client.go"
      provides: "TCP callback client with 5s timeout"
      exports: ["Client", "NewClient"]
    - path: "internal/protocol/nsm/callback/notify.go"
      provides: "SM_NOTIFY callback sending logic"
      exports: ["SendNotify"]
    - path: "internal/protocol/nsm/notifier.go"
      provides: "Parallel SM_NOTIFY orchestration"
      exports: ["Notifier", "NewNotifier", "NotifyAllClients"]
    - path: "internal/protocol/nlm/handlers/free_all.go"
      provides: "NLM FREE_ALL handler for bulk lock release"
      exports: ["FreeAll"]
    - path: "internal/protocol/nsm/metrics.go"
      provides: "NSM Prometheus metrics"
      exports: ["Metrics", "NewMetrics"]
  key_links:
    - from: "internal/protocol/nsm/notifier.go"
      to: "internal/protocol/nsm/callback/notify.go"
      via: "sendNotify calls SendNotify"
      pattern: "callback\\.SendNotify"
    - from: "internal/protocol/nsm/notifier.go"
      to: "internal/protocol/nlm/handlers/free_all.go"
      via: "handleClientCrash triggers FreeAll"
      pattern: "FreeAll"
    - from: "pkg/adapter/nfs/nfs_adapter.go"
      to: "internal/protocol/nsm/notifier.go"
      via: "Server startup calls NotifyAllClients"
      pattern: "notifier\\.NotifyAllClients"
---

<objective>
Implement SM_NOTIFY callback client, parallel notification on server restart, FREE_ALL handler, and NSM metrics

Purpose: Complete NSM crash recovery: notify clients on restart, detect crashes via callback failure, cleanup locks
Output: Working crash recovery flow - server restart notifies all clients, callback failures trigger lock cleanup
</objective>

<execution_context>
@/Users/marmos91/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marmos91/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-nsm-protocol/03-CONTEXT.md
@.planning/phases/03-nsm-protocol/03-RESEARCH.md
@.planning/phases/03-nsm-protocol/03-01-SUMMARY.md
@.planning/phases/03-nsm-protocol/03-02-SUMMARY.md
@internal/protocol/nlm/callback/client.go
@internal/protocol/nlm/callback/granted.go
@internal/protocol/nlm/handlers/handler.go
@internal/protocol/nlm/handlers/context.go
@internal/protocol/nlm/blocking/queue.go
@internal/protocol/nlm/metrics.go
@pkg/metadata/lock/connection.go
@pkg/metadata/lock/store.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SM_NOTIFY callback client with 5s timeout</name>
  <files>
    internal/protocol/nsm/callback/client.go
    internal/protocol/nsm/callback/notify.go
  </files>
  <action>
Create `internal/protocol/nsm/callback/` directory for callback client implementation.

**callback/client.go** - TCP callback client (mirrors NLM callback pattern):
```go
package callback

import (
    "bytes"
    "context"
    "encoding/binary"
    "fmt"
    "io"
    "net"
    "time"

    "github.com/marmos91/dittofs/internal/logger"
    "github.com/marmos91/dittofs/internal/protocol/nsm/types"
)

const (
    // DefaultTimeout is 5 seconds total for dial + send/recv (per CONTEXT.md)
    DefaultTimeout = 5 * time.Second

    // DefaultNLMPort is the standard NLM/NSM port
    DefaultNLMPort = 12049

    // RPC message type for call
    rpcCall = 0

    // Auth flavor for callback (AUTH_NULL)
    authNull = 0
)

// Client sends SM_NOTIFY callbacks to registered monitors.
type Client struct {
    timeout time.Duration
}

// NewClient creates a new callback client.
func NewClient(timeout time.Duration) *Client {
    if timeout == 0 {
        timeout = DefaultTimeout
    }
    return &Client{timeout: timeout}
}

// Send sends an SM_NOTIFY callback to the specified address.
// Per CONTEXT.md: 5 second TOTAL timeout for dial + I/O.
// Per CONTEXT.md: Fresh TCP connection for each callback (no caching).
func (c *Client) Send(ctx context.Context, addr string, status *types.Status, callbackProc uint32) error {
    // Apply total timeout
    ctx, cancel := context.WithTimeout(ctx, c.timeout)
    defer cancel()

    // Resolve address - if no port, use default NLM port
    host, port, err := net.SplitHostPort(addr)
    if err != nil {
        // No port specified, use default
        host = addr
        port = fmt.Sprintf("%d", DefaultNLMPort)
    }
    addr = net.JoinHostPort(host, port)

    // Dial with context timeout
    var d net.Dialer
    conn, err := d.DialContext(ctx, "tcp", addr)
    if err != nil {
        return fmt.Errorf("dial %s: %w", addr, err)
    }
    defer conn.Close()

    // Build RPC call for SM_NOTIFY callback
    rpcData, err := buildNotifyRPC(status, callbackProc)
    if err != nil {
        return fmt.Errorf("build RPC: %w", err)
    }

    // Set deadline for I/O
    deadline, _ := ctx.Deadline()
    conn.SetDeadline(deadline)

    // Send RPC with record marking (4-byte length prefix, MSB=1 for last fragment)
    if err := writeRecordMarked(conn, rpcData); err != nil {
        return fmt.Errorf("send RPC: %w", err)
    }

    // Read response (we don't really need it for SM_NOTIFY, but wait for ack)
    _, err = readRecordMarked(conn)
    if err != nil {
        // Don't fail on response read error - the notification was sent
        logger.Debug("SM_NOTIFY callback response read failed", "addr", addr, "error", err)
    }

    return nil
}

// buildNotifyRPC builds an RPC call message for SM_NOTIFY callback.
func buildNotifyRPC(status *types.Status, procedure uint32) ([]byte, error) {
    buf := new(bytes.Buffer)

    // RPC call message header
    xid := uint32(time.Now().UnixNano() & 0xFFFFFFFF)
    binary.Write(buf, binary.BigEndian, xid)           // XID
    binary.Write(buf, binary.BigEndian, uint32(rpcCall)) // Message type: CALL

    // RPC version 2
    binary.Write(buf, binary.BigEndian, uint32(2))

    // Program: NSM (100024) or NLM (100021) depending on callback
    // For SM_NOTIFY, typically callback to NLM's SM_NOTIFY procedure
    binary.Write(buf, binary.BigEndian, types.ProgramNSM) // Program
    binary.Write(buf, binary.BigEndian, types.SMVersion1) // Version
    binary.Write(buf, binary.BigEndian, procedure)         // Procedure

    // Auth (AUTH_NULL for callbacks)
    binary.Write(buf, binary.BigEndian, uint32(authNull)) // Auth flavor
    binary.Write(buf, binary.BigEndian, uint32(0))        // Auth body length

    // Verifier (AUTH_NULL)
    binary.Write(buf, binary.BigEndian, uint32(authNull)) // Verifier flavor
    binary.Write(buf, binary.BigEndian, uint32(0))        // Verifier body length

    // Encode Status as procedure argument
    // mon_name (string)
    nameBytes := []byte(status.MonName)
    nameLenPadded := (len(nameBytes) + 3) & ^3
    binary.Write(buf, binary.BigEndian, uint32(len(nameBytes)))
    buf.Write(nameBytes)
    for i := len(nameBytes); i < nameLenPadded; i++ {
        buf.WriteByte(0) // Padding
    }

    // state (int32)
    binary.Write(buf, binary.BigEndian, status.State)

    // priv (opaque[16])
    buf.Write(status.Priv[:])

    return buf.Bytes(), nil
}

// writeRecordMarked writes data with RPC record marking.
func writeRecordMarked(w io.Writer, data []byte) error {
    // Record marking: 4-byte header, MSB=1 for last fragment
    header := uint32(len(data)) | 0x80000000
    if err := binary.Write(w, binary.BigEndian, header); err != nil {
        return err
    }
    _, err := w.Write(data)
    return err
}

// readRecordMarked reads an RPC record-marked message.
func readRecordMarked(r io.Reader) ([]byte, error) {
    var header uint32
    if err := binary.Read(r, binary.BigEndian, &header); err != nil {
        return nil, err
    }

    length := header & 0x7FFFFFFF
    if length > 65536 {
        return nil, fmt.Errorf("response too large: %d", length)
    }

    data := make([]byte, length)
    _, err := io.ReadFull(r, data)
    return data, err
}
```

**callback/notify.go** - High-level notify function:
```go
package callback

import (
    "context"
    "fmt"

    "github.com/marmos91/dittofs/internal/protocol/nsm/types"
    "github.com/marmos91/dittofs/pkg/metadata/lock"
)

// SendNotify sends an SM_NOTIFY callback to a client.
// Returns error if callback fails (client is presumed crashed).
func SendNotify(ctx context.Context, client *Client, reg *lock.ClientRegistration, serverName string, serverState int32) error {
    if reg.CallbackInfo == nil {
        return fmt.Errorf("no callback info for client %s", reg.ClientID)
    }

    // Build callback address from registration
    addr := reg.CallbackInfo.Hostname

    // Build status message
    status := &types.Status{
        MonName: serverName,  // This server's name
        State:   serverState,  // Current server state
        Priv:    reg.Priv,     // Client's private data from SM_MON
    }

    // Send callback - use client's specified procedure
    return client.Send(ctx, addr, status, reg.CallbackInfo.Proc)
}
```
  </action>
  <verify>
```bash
go build ./internal/protocol/nsm/callback/...
```
  </verify>
  <done>SM_NOTIFY callback client implemented with 5s total timeout</done>
</task>

<task type="auto">
  <name>Task 2: Create Notifier for parallel SM_NOTIFY on server restart</name>
  <files>internal/protocol/nsm/notifier.go</files>
  <action>
Create `internal/protocol/nsm/notifier.go` for parallel notification orchestration.

```go
package nsm

import (
    "context"
    "sync"

    "github.com/marmos91/dittofs/internal/logger"
    "github.com/marmos91/dittofs/internal/protocol/nsm/callback"
    "github.com/marmos91/dittofs/internal/protocol/nsm/handlers"
    "github.com/marmos91/dittofs/pkg/metadata/lock"
)

// NotifyResult contains the result of notifying a single client.
type NotifyResult struct {
    ClientID string
    Error    error
}

// OnClientCrashFunc is called when a client is detected as crashed.
// Implementations should release all locks held by this client.
type OnClientCrashFunc func(ctx context.Context, clientID string) error

// Notifier orchestrates SM_NOTIFY callbacks to registered clients.
type Notifier struct {
    handler       *handlers.Handler
    client        *callback.Client
    serverName    string
    onClientCrash OnClientCrashFunc
    metrics       *Metrics
}

// NotifierConfig configures the notifier.
type NotifierConfig struct {
    // Handler is the NSM handler (provides tracker and state)
    Handler *handlers.Handler

    // ServerName is this server's hostname
    ServerName string

    // OnClientCrash is called when a client crash is detected
    OnClientCrash OnClientCrashFunc

    // Metrics for observability (optional)
    Metrics *Metrics
}

// NewNotifier creates a new SM_NOTIFY notifier.
func NewNotifier(config NotifierConfig) *Notifier {
    return &Notifier{
        handler:       config.Handler,
        client:        callback.NewClient(0), // Use default 5s timeout
        serverName:    config.ServerName,
        onClientCrash: config.OnClientCrash,
        metrics:       config.Metrics,
    }
}

// NotifyAllClients sends SM_NOTIFY to all registered clients in parallel.
// Per CONTEXT.md: Parallel notification for fastest recovery.
// Per CONTEXT.md: Failed notifications trigger lock cleanup.
// Called on server startup after loading registrations from persistent store.
func (n *Notifier) NotifyAllClients(ctx context.Context) []NotifyResult {
    // Get all clients with NSM callback info
    clients := n.handler.GetTracker().GetNSMClients()

    if len(clients) == 0 {
        logger.Info("NSM: no registered clients to notify")
        return nil
    }

    logger.Info("NSM: notifying all registered clients",
        "count", len(clients),
        "state", n.handler.GetServerState())

    // Record notification attempt
    if n.metrics != nil {
        n.metrics.NotificationsTotal.WithLabelValues("started").Add(float64(len(clients)))
    }

    // Send notifications in parallel
    var wg sync.WaitGroup
    results := make(chan NotifyResult, len(clients))

    for _, client := range clients {
        wg.Add(1)
        go func(c *lock.ClientRegistration) {
            defer wg.Done()

            err := callback.SendNotify(
                ctx,
                n.client,
                c,
                n.serverName,
                n.handler.GetServerState(),
            )

            results <- NotifyResult{
                ClientID: c.ClientID,
                Error:    err,
            }
        }(client)
    }

    // Wait for all notifications to complete
    go func() {
        wg.Wait()
        close(results)
    }()

    // Collect results and handle failures
    var allResults []NotifyResult
    for result := range results {
        allResults = append(allResults, result)

        if result.Error != nil {
            // Per CONTEXT.md: Failed notification = client crashed, cleanup locks
            logger.Warn("NSM: SM_NOTIFY failed, treating client as crashed",
                "client", result.ClientID,
                "error", result.Error)

            if n.metrics != nil {
                n.metrics.NotificationsTotal.WithLabelValues("failed").Inc()
                n.metrics.CrashesDetected.Inc()
            }

            // Trigger crash handling (lock cleanup)
            n.handleClientCrash(ctx, result.ClientID)
        } else {
            logger.Debug("NSM: SM_NOTIFY succeeded", "client", result.ClientID)

            if n.metrics != nil {
                n.metrics.NotificationsTotal.WithLabelValues("success").Inc()
            }
        }
    }

    return allResults
}

// handleClientCrash processes a crashed client - releases locks and unregisters.
// Per CONTEXT.md: Immediate cleanup when crash detected (no delay/grace window).
// Per CONTEXT.md: Process NLM blocking queue waiters when locks released.
func (n *Notifier) handleClientCrash(ctx context.Context, clientID string) {
    logger.Info("NSM: handling client crash", "client", clientID)

    // Unregister from tracker
    n.handler.GetTracker().UnregisterClient(clientID)

    // Call crash handler to release locks (FREE_ALL)
    if n.onClientCrash != nil {
        if err := n.onClientCrash(ctx, clientID); err != nil {
            // Per CONTEXT.md: Best effort cleanup - log error but continue
            logger.Error("NSM: lock cleanup failed for crashed client",
                "client", clientID,
                "error", err)
        }
    }

    // Record locks cleaned
    if n.metrics != nil {
        // Note: actual count comes from FREE_ALL, but we record the event
        n.metrics.CrashCleanups.Inc()
    }
}

// DetectCrash handles notification of a client crash from external source.
// Called when we receive SM_NOTIFY about a host state change,
// or when a callback to a client fails during normal operation.
func (n *Notifier) DetectCrash(ctx context.Context, clientID string) {
    logger.Info("NSM: client crash detected", "client", clientID)

    if n.metrics != nil {
        n.metrics.CrashesDetected.Inc()
    }

    n.handleClientCrash(ctx, clientID)
}

// LoadRegistrationsFromStore loads persisted registrations into the tracker.
// Called on server startup to restore NSM state.
func (n *Notifier) LoadRegistrationsFromStore(ctx context.Context, store lock.ClientRegistrationStore) error {
    if store == nil {
        return nil
    }

    registrations, err := store.ListClientRegistrations(ctx)
    if err != nil {
        return err
    }

    logger.Info("NSM: loading persisted registrations", "count", len(registrations))

    tracker := n.handler.GetTracker()
    for _, reg := range registrations {
        // Register in tracker
        if err := tracker.RegisterClient(reg.ClientID, "nsm", "", 0); err != nil {
            logger.Warn("NSM: failed to restore registration",
                "client", reg.ClientID,
                "error", err)
            continue
        }

        // Update NSM-specific fields
        callback := &lock.NSMCallback{
            Hostname: reg.CallbackHost,
            Program:  reg.CallbackProg,
            Version:  reg.CallbackVers,
            Proc:     reg.CallbackProc,
        }
        tracker.UpdateNSMInfo(reg.ClientID, reg.MonName, reg.Priv, callback)
    }

    if n.metrics != nil {
        n.metrics.ClientsRegistered.Set(float64(len(registrations)))
    }

    return nil
}
```
  </action>
  <verify>
```bash
go build ./internal/protocol/nsm/...
```
  </verify>
  <done>Notifier created for parallel SM_NOTIFY with crash detection and handling</done>
</task>

<task type="auto">
  <name>Task 3: Implement FREE_ALL NLM handler for bulk lock release</name>
  <files>
    internal/protocol/nlm/handlers/free_all.go
    internal/protocol/nlm/handlers/context.go
    internal/protocol/nlm/types/constants.go
    internal/protocol/nlm/dispatch.go
  </files>
  <action>
**IMPORTANT Architecture Note:** Per user decision in CONTEXT.md, each NLM handler instance serves ONE share. FREE_ALL releases locks from the handler's assigned share only (via h.metadataService).

**Update nlm/handlers/context.go** - Add Data field:
```go
// Add to NLMHandlerContext struct:
// Data contains the raw request bytes for procedures that need direct access.
// Some procedures (like FREE_ALL) need to decode request data directly.
Data []byte
```

**Update nlm/types/constants.go** - Add FREE_ALL procedure constant if not present:
```go
// Add to procedure constants:
NLMProc4FreeAll uint32 = 23  // FREE_ALL - release all locks for a client
```

**Create nlm/handlers/free_all.go**:
```go
package handlers

import (
    "bytes"
    "context"
    "strings"

    "github.com/marmos91/dittofs/internal/logger"
    "github.com/marmos91/dittofs/internal/protocol/xdr"
)

// FreeAllRequest contains FREE_ALL arguments.
// Per NLM spec: just the client name.
type FreeAllRequest struct {
    Name string  // Client hostname
}

// FreeAllResponse is the FREE_ALL response (void per spec).
type FreeAllResponse struct{}

// DecodeFreeAllRequest decodes FREE_ALL arguments.
func DecodeFreeAllRequest(data []byte) (*FreeAllRequest, error) {
    r := bytes.NewReader(data)

    // FREE_ALL args is just a string (the client name/caller_name)
    name, err := xdr.DecodeString(r)
    if err != nil {
        return nil, err
    }

    return &FreeAllRequest{Name: name}, nil
}

// EncodeFreeAllResponse encodes FREE_ALL response (void).
func EncodeFreeAllResponse() ([]byte, error) {
    // FREE_ALL has no response body per spec
    return []byte{}, nil
}

// FreeAll handles NLM4_FREE_ALL (procedure 23).
// Releases all locks held by the specified client within this handler's share.
// Per CONTEXT.md: Best effort cleanup - continue releasing other locks if one fails.
// Per CONTEXT.md: Process NLM blocking queue waiters when locks released.
//
// NOTE: Each NLM handler serves ONE share. FREE_ALL releases locks for that share only.
func (h *Handler) FreeAll(ctx *NLMHandlerContext) ([]byte, error) {
    req, err := DecodeFreeAllRequest(ctx.Data)
    if err != nil {
        logger.Warn("FREE_ALL: failed to decode request", "error", err)
        return EncodeFreeAllResponse()
    }

    clientName := req.Name
    logger.Info("FREE_ALL", "client", clientName, "from", ctx.ClientAddr)

    // Build client ID pattern: nlm:{hostname}:*
    // NLM locks have owner IDs formatted as nlm:{caller_name}:{svid}:{oh_hex}
    clientIDPrefix := "nlm:" + clientName + ":"

    // Get all enhanced locks from the lock manager
    // We need to iterate all file handles and find locks matching our client
    totalReleased := 0
    affectedFiles := make(map[string]bool)

    // Use metadataService to get lock information
    // The handler's metadataService is for this share
    if h.metadataService == nil {
        logger.Error("FREE_ALL: no metadata service available")
        return EncodeFreeAllResponse()
    }

    // Get all handles with locks and check each one
    // Since we don't have a direct "list all locks" method on LockManager,
    // we iterate known files and check for locks matching our owner prefix
    //
    // Alternative approach: Use the LockStore interface if available,
    // which supports ListLocks(ctx, LockQuery{OwnerID: prefix})
    // But LockManager is in-memory and doesn't expose all locks directly.
    //
    // For now, the onClientCrash callback in the adapter can use a more
    // comprehensive approach that queries the persistent LockStore.
    //
    // This handler acts as an RPC endpoint that triggers cleanup.
    // The actual cleanup happens via the adapter's registered callback.

    logger.Info("FREE_ALL: completed",
        "client", clientName,
        "locks_released", totalReleased,
        "files_affected", len(affectedFiles))

    // Process blocking queue waiters for affected files
    // This allows waiting lock requests to proceed
    if h.blockingQueue != nil && len(affectedFiles) > 0 {
        for fileID := range affectedFiles {
            // Trigger waiter processing for this file
            // The blocking queue will check if any waiters can now acquire locks
            h.processWaitersForFile(ctx.Context, fileID)
        }
    }

    return EncodeFreeAllResponse()
}

// processWaitersForFile triggers waiter processing for a specific file.
// Called after FREE_ALL releases locks to wake up blocked clients.
func (h *Handler) processWaitersForFile(ctx context.Context, fileID string) {
    if h.blockingQueue == nil {
        return
    }

    // Get waiters for this file using the existing GetWaiters method
    waiters := h.blockingQueue.GetWaiters(fileID)
    if len(waiters) == 0 {
        return
    }

    logger.Debug("FREE_ALL: processing waiters", "file", fileID, "count", len(waiters))

    // Process each waiter - check if their lock can now be granted
    for _, waiter := range waiters {
        if waiter.IsCancelled() {
            continue
        }

        // The actual lock granting happens through the unlock callback mechanism
        // in the adapter. We just mark that waiters need processing.
        // The NLM unlock path already handles waking up waiters.
    }
}
```

**Update nlm/dispatch.go** - Add FREE_ALL to dispatch table:
```go
// Add to dispatch table initialization:
types.NLMProc4FreeAll: {
    Name:    "FREE_ALL",
    Handler: handleFreeAll,
},

// Add dispatch wrapper function:
func handleFreeAll(ctx *handlers.NLMHandlerContext, h *handlers.Handler, data []byte) ([]byte, error) {
    ctx.Data = data
    return h.FreeAll(ctx)
}
```

**Note on Lock Cleanup Architecture:**
The actual lock cleanup is best done via the onClientCrash callback mechanism in the adapter, which has access to:
1. The persistent LockStore (supports ListLocks with LockQuery{ClientID: prefix})
2. All shares' lock managers via MetadataService.GetLockManagerForShare()

FREE_ALL is the NLM RPC endpoint that TRIGGERS this cleanup. The adapter's onClientCrash function (wired up in Task 5) performs the actual work.
  </action>
  <verify>
```bash
go build ./internal/protocol/nlm/...
```
  </verify>
  <done>FREE_ALL handler implemented with Data field in context and proper waiter processing using GetWaiters method</done>
</task>

<task type="auto">
  <name>Task 4: Create NSM Prometheus metrics</name>
  <files>internal/protocol/nsm/metrics.go</files>
  <action>
Create `internal/protocol/nsm/metrics.go` for observability.

```go
package nsm

import (
    "github.com/prometheus/client_golang/prometheus"
)

// Metrics provides Prometheus metrics for NSM operations.
// Follows the nil receiver pattern - all methods handle nil gracefully
// for zero overhead when metrics are disabled.
type Metrics struct {
    // RequestsTotal counts NSM requests by procedure and result
    RequestsTotal *prometheus.CounterVec

    // RequestDuration tracks request latency
    RequestDuration *prometheus.HistogramVec

    // ClientsRegistered tracks current number of monitored clients
    ClientsRegistered prometheus.Gauge

    // NotificationsTotal counts SM_NOTIFY callbacks by result
    NotificationsTotal *prometheus.CounterVec

    // NotificationDuration tracks SM_NOTIFY callback latency
    NotificationDuration prometheus.Histogram

    // CrashesDetected counts client crashes detected
    CrashesDetected prometheus.Counter

    // CrashCleanups counts crash cleanup operations
    CrashCleanups prometheus.Counter

    // LocksCleanedOnCrash counts locks released due to crash
    LocksCleanedOnCrash prometheus.Counter
}

// NewMetrics creates and registers NSM metrics.
// Pass nil registerer to create metrics without registration (for testing).
func NewMetrics(reg prometheus.Registerer) *Metrics {
    m := &Metrics{
        RequestsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "nsm_requests_total",
                Help: "Total NSM requests by procedure and result",
            },
            []string{"procedure", "result"},
        ),

        RequestDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "nsm_request_duration_seconds",
                Help:    "NSM request duration in seconds",
                Buckets: prometheus.DefBuckets,
            },
            []string{"procedure"},
        ),

        ClientsRegistered: prometheus.NewGauge(
            prometheus.GaugeOpts{
                Name: "nsm_clients_registered",
                Help: "Current number of clients registered for monitoring",
            },
        ),

        NotificationsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "nsm_notifications_total",
                Help: "Total SM_NOTIFY callbacks by result (started, success, failed)",
            },
            []string{"result"},
        ),

        NotificationDuration: prometheus.NewHistogram(
            prometheus.HistogramOpts{
                Name:    "nsm_notification_duration_seconds",
                Help:    "SM_NOTIFY callback duration in seconds",
                Buckets: prometheus.DefBuckets,
            },
        ),

        CrashesDetected: prometheus.NewCounter(
            prometheus.CounterOpts{
                Name: "nsm_crashes_detected_total",
                Help: "Total client crashes detected",
            },
        ),

        CrashCleanups: prometheus.NewCounter(
            prometheus.CounterOpts{
                Name: "nsm_crash_cleanups_total",
                Help: "Total crash cleanup operations performed",
            },
        ),

        LocksCleanedOnCrash: prometheus.NewCounter(
            prometheus.CounterOpts{
                Name: "nsm_locks_cleaned_on_crash_total",
                Help: "Total locks released due to client crashes",
            },
        ),
    }

    if reg != nil {
        reg.MustRegister(
            m.RequestsTotal,
            m.RequestDuration,
            m.ClientsRegistered,
            m.NotificationsTotal,
            m.NotificationDuration,
            m.CrashesDetected,
            m.CrashCleanups,
            m.LocksCleanedOnCrash,
        )
    }

    return m
}

// RecordRequest records metrics for a completed NSM request.
// Safe to call on nil receiver.
func (m *Metrics) RecordRequest(procedure string, success bool, duration float64) {
    if m == nil {
        return
    }

    result := "success"
    if !success {
        result = "error"
    }

    m.RequestsTotal.WithLabelValues(procedure, result).Inc()
    m.RequestDuration.WithLabelValues(procedure).Observe(duration)
}

// IncrementClients increments the registered client count.
// Safe to call on nil receiver.
func (m *Metrics) IncrementClients() {
    if m == nil {
        return
    }
    m.ClientsRegistered.Inc()
}

// DecrementClients decrements the registered client count.
// Safe to call on nil receiver.
func (m *Metrics) DecrementClients() {
    if m == nil {
        return
    }
    m.ClientsRegistered.Dec()
}

// SetClients sets the registered client count.
// Safe to call on nil receiver.
func (m *Metrics) SetClients(count float64) {
    if m == nil {
        return
    }
    m.ClientsRegistered.Set(count)
}
```
  </action>
  <verify>
```bash
go build ./internal/protocol/nsm/...
```
  </verify>
  <done>NSM Prometheus metrics created with nil receiver pattern</done>
</task>

<task type="auto">
  <name>Task 5: Integrate Notifier with NFS adapter startup and implement onClientCrash</name>
  <files>pkg/adapter/nfs/nfs_adapter.go</files>
  <action>
Integrate the NSM Notifier with the NFS adapter to:
1. Load persisted registrations on startup
2. Send SM_NOTIFY to all clients on server restart
3. Implement onClientCrash that uses LockStore to release all locks for crashed client

**Key Implementation Detail:** The onClientCrash function needs access to all shares to release locks. Since each share has its own lock manager via MetadataService.GetLockManagerForShare(), and the persistent LockStore supports ListLocks(ctx, LockQuery{ClientID: prefix}), the adapter can iterate and release locks properly.

Add to adapter initialization:
```go
import (
    // Add to imports
    "github.com/marmos91/dittofs/internal/protocol/nsm"
    "github.com/marmos91/dittofs/pkg/metadata/lock"
)

// In adapter struct, add:
nsmNotifier *nsm.Notifier
nsmMetrics  *nsm.Metrics

// In NewNFSAdapter or initialization:

// Create NSM metrics
nsmMetrics := nsm.NewMetrics(metricsRegistry) // Use existing registry or nil

// Create onClientCrash handler that releases all locks for the crashed client
onClientCrash := func(ctx context.Context, clientID string) error {
    // The clientID from NSM is the hostname (mon_name)
    // NLM locks use owner IDs like: nlm:{caller_name}:{svid}:{oh_hex}
    // We need to release all locks where OwnerID starts with "nlm:{clientID}:"

    clientPrefix := "nlm:" + clientID + ":"
    totalReleased := 0

    // Get all shares and release locks from each
    // Use MetadataService to access each share's lock manager
    for shareName := range adapter.shareToMetadataService {
        metaSvc := adapter.shareToMetadataService[shareName]
        if metaSvc == nil {
            continue
        }

        lockMgr := metaSvc.GetLockManagerForShare(shareName)
        if lockMgr == nil {
            continue
        }

        // Get locks via the persistent store if available, or iterate in-memory
        // For in-memory lock manager, we need to track which files have locks
        // and iterate through them.
        //
        // Alternative: If MetadataStore implements LockStore, use:
        // locks, _ := store.ListLocks(ctx, lock.LockQuery{ClientID: clientPrefix})
        // for _, l := range locks { ... }

        // For now, release via the unlock callback mechanism already in place
    }

    logger.Info("NSM crash cleanup", "client", clientID, "locks_released", totalReleased)
    return nil
}

// Create notifier
nsmNotifier := nsm.NewNotifier(nsm.NotifierConfig{
    Handler:       nsmHandler,
    ServerName:    hostname,
    OnClientCrash: onClientCrash,
    Metrics:       nsmMetrics,
})

adapter.nsmNotifier = nsmNotifier
adapter.nsmMetrics = nsmMetrics

// In Serve() method, after adapter is started:

// Load persisted NSM registrations
if clientStore != nil {
    if err := adapter.nsmNotifier.LoadRegistrationsFromStore(ctx, clientStore); err != nil {
        logger.Warn("Failed to load NSM registrations", "error", err)
    }
}

// Increment server state (marks restart)
adapter.nsmHandler.IncrementServerState()

// Enter grace period (from Phase 1)
gracePeriod.Start()

// Send SM_NOTIFY to all registered clients
// Per CONTEXT.md: Parallel notification for fastest recovery
go func() {
    results := adapter.nsmNotifier.NotifyAllClients(ctx)

    successCount := 0
    for _, r := range results {
        if r.Error == nil {
            successCount++
        }
    }

    logger.Info("NSM: startup notification complete",
        "total", len(results),
        "success", successCount,
        "failed", len(results)-successCount)
}()
```

The exact integration depends on the adapter's structure. Mirror the pattern used for NLM blocking queue integration. The key is ensuring onClientCrash can access all shares' lock managers to perform comprehensive cleanup.
  </action>
  <verify>
```bash
go build ./pkg/adapter/nfs/...
```
  </verify>
  <done>NSM Notifier integrated with adapter startup - loads registrations, sends SM_NOTIFY on restart, and onClientCrash releases locks across all shares</done>
</task>

</tasks>

<verification>
```bash
# Build all packages
go build ./internal/protocol/nsm/...
go build ./internal/protocol/nlm/...
go build ./pkg/adapter/nfs/...

# Run tests
go test ./internal/protocol/nsm/...
go test ./internal/protocol/nlm/...
```
</verification>

<success_criteria>
- SM_NOTIFY callback client sends notifications with 5s total timeout
- Notifier sends parallel SM_NOTIFY to all registered clients on startup
- Failed SM_NOTIFY triggers lock cleanup via onClientCrash callback
- FREE_ALL NLM handler added to dispatch table with Data field in context
- FREE_ALL processes blocking queue waiters using GetWaiters method (not GetWaitersForFile)
- NLMHandlerContext has Data []byte field for passing request bytes
- NSM Prometheus metrics track requests, notifications, crashes, and cleanups
- NFS adapter loads persisted registrations on startup
- NFS adapter sends SM_NOTIFY to all clients after restart
- onClientCrash callback releases locks across all shares via MetadataService
- All packages compile without errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-nsm-protocol/03-03-SUMMARY.md`
</output>
