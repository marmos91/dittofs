---
phase: 05-status-conditions-and-lifecycle
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go
  - k8s/dittofs-operator/internal/controller/dittoserver_controller.go
  - k8s/dittofs-operator/utils/conditions/conditions.go
autonomous: true

must_haves:
  truths:
    - "kubectl get dittofs shows READY, STATUS columns with accurate values"
    - "kubectl get dittofs -o yaml shows five conditions: Ready, Available, ConfigReady, DatabaseReady, Progressing"
    - "Conditions have correct observedGeneration matching metadata.generation when reconciled"
    - "Status shows replica counts: replicas, readyReplicas, availableReplicas"
    - "ConfigHash visible in status for debugging"
  artifacts:
    - path: "k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go"
      provides: "Enhanced DittoServerStatus with all condition fields"
      contains: "ObservedGeneration int64"
    - path: "k8s/dittofs-operator/internal/controller/dittoserver_controller.go"
      provides: "Reconciler that sets all five conditions based on resource state"
      contains: "ConditionConfigReady"
    - path: "k8s/dittofs-operator/utils/conditions/conditions.go"
      provides: "Condition type constants"
      contains: "ConditionReady"
  key_links:
    - from: "dittoserver_controller.go"
      to: "conditions.go"
      via: "SetCondition calls with condition type constants"
      pattern: "conditions\\.SetCondition.*Condition"
---

<objective>
Implement comprehensive status conditions for DittoServer CRD to provide full observability of server state.

Purpose: Enable users to understand DittoServer health at a glance via kubectl and programmatically via status conditions. Required for production readiness (R5.1).

Output: Enhanced DittoServerStatus with five condition types (Ready, Available, ConfigReady, DatabaseReady, Progressing), observedGeneration tracking, replica counts, and configHash. Updated kubectl print columns showing replica counts and phase.
</objective>

<execution_context>
@/Users/marmos91/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marmos91/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-status-conditions-and-lifecycle/05-RESEARCH.md
@k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go
@k8s/dittofs-operator/internal/controller/dittoserver_controller.go
@k8s/dittofs-operator/utils/conditions/conditions.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance DittoServerStatus struct and add condition type constants</name>
  <files>
    k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go
    k8s/dittofs-operator/utils/conditions/conditions.go
  </files>
  <action>
1. In `utils/conditions/conditions.go`, add condition type constants after the existing imports and before SetCondition:

```go
// DittoServer condition types
const (
    // ConditionReady indicates the DittoServer is fully operational
    ConditionReady = "Ready"

    // ConditionAvailable indicates the StatefulSet has minimum ready replicas
    ConditionAvailable = "Available"

    // ConditionConfigReady indicates ConfigMap and secrets are valid
    ConditionConfigReady = "ConfigReady"

    // ConditionDatabaseReady indicates PostgreSQL (Percona) is ready (when enabled)
    ConditionDatabaseReady = "DatabaseReady"

    // ConditionProgressing indicates a change is being applied
    ConditionProgressing = "Progressing"
)
```

2. In `dittoserver_types.go`, replace the existing DittoServerStatus struct with:

```go
// DittoServerStatus defines the observed state of DittoServer
type DittoServerStatus struct {
    // ObservedGeneration is the generation last processed by the controller
    ObservedGeneration int64 `json:"observedGeneration,omitempty"`

    // Replicas is the desired number of replicas
    Replicas int32 `json:"replicas,omitempty"`

    // ReadyReplicas is the number of pods with Ready condition
    ReadyReplicas int32 `json:"readyReplicas,omitempty"`

    // AvailableReplicas is the number of pods ready for at least minReadySeconds
    AvailableReplicas int32 `json:"availableReplicas,omitempty"`

    // NFSEndpoint that clients should use to mount
    // Format: service-name.namespace.svc.cluster.local:2049
    NFSEndpoint string `json:"nfsEndpoint,omitempty"`

    // Phase of the DittoServer (Pending, Running, Failed, Stopped, Deleting)
    // +kubebuilder:validation:Enum=Pending;Running;Failed;Stopped;Deleting
    Phase string `json:"phase,omitempty"`

    // ConfigHash is the hash of current configuration (for debugging)
    ConfigHash string `json:"configHash,omitempty"`

    // PerconaClusterName is the name of the owned PerconaPGCluster (when enabled)
    // +optional
    PerconaClusterName string `json:"perconaClusterName,omitempty"`

    // Conditions represent the latest available observations
    // +listType=map
    // +listMapKey=type
    // +optional
    Conditions []metav1.Condition `json:"conditions,omitempty"`
}
```

3. Update the kubebuilder print column markers on the DittoServer struct. Replace the existing markers with:

```go
// +kubebuilder:printcolumn:name="Replicas",type=integer,JSONPath=`.spec.replicas`
// +kubebuilder:printcolumn:name="Ready",type=integer,JSONPath=`.status.readyReplicas`
// +kubebuilder:printcolumn:name="Available",type=integer,JSONPath=`.status.availableReplicas`
// +kubebuilder:printcolumn:name="Status",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
```

Note: Remove the existing "NFS Endpoint" print column - it's too long for kubectl output. Keep NFSEndpoint in status for programmatic access.
  </action>
  <verify>
Run from k8s/dittofs-operator:
```bash
go build ./...
```
Build succeeds with no errors.
  </verify>
  <done>
- DittoServerStatus has: ObservedGeneration, Replicas, ReadyReplicas, AvailableReplicas, Phase, ConfigHash, PerconaClusterName, Conditions
- Condition constants defined: ConditionReady, ConditionAvailable, ConditionConfigReady, ConditionDatabaseReady, ConditionProgressing
- Print columns show: Replicas, Ready, Available, Status, Age
  </done>
</task>

<task type="auto">
  <name>Task 2: Update reconciler to set all five conditions based on resource state</name>
  <files>
    k8s/dittofs-operator/internal/controller/dittoserver_controller.go
  </files>
  <action>
Update the reconciler to set all five conditions during reconciliation. Modify the status update section at the end of Reconcile():

1. Import the conditions package at the top:
```go
"github.com/marmos91/dittofs/k8s/dittofs-operator/utils/conditions"
```

2. Add a helper function to update ConfigReady condition. Add this as a new method on the reconciler:
```go
// updateConfigReadyCondition checks ConfigMap and sets ConfigReady condition.
func (r *DittoServerReconciler) updateConfigReadyCondition(ctx context.Context, dittoServer *dittoiov1alpha1.DittoServer, statusCopy *dittoiov1alpha1.DittoServerStatus) {
    configMap := &corev1.ConfigMap{}
    err := r.Get(ctx, client.ObjectKey{
        Namespace: dittoServer.Namespace,
        Name:      dittoServer.Name + "-config",
    }, configMap)

    if err != nil {
        conditions.SetCondition(&statusCopy.Conditions, dittoServer.Generation,
            conditions.ConditionConfigReady, metav1.ConditionFalse, "ConfigMapNotFound",
            fmt.Sprintf("ConfigMap %s-config not found: %v", dittoServer.Name, err))
        return
    }

    if _, ok := configMap.Data["config.yaml"]; !ok {
        conditions.SetCondition(&statusCopy.Conditions, dittoServer.Generation,
            conditions.ConditionConfigReady, metav1.ConditionFalse, "ConfigMissing",
            "ConfigMap does not contain config.yaml key")
        return
    }

    conditions.SetCondition(&statusCopy.Conditions, dittoServer.Generation,
        conditions.ConditionConfigReady, metav1.ConditionTrue, "ConfigValid",
        "ConfigMap is valid and contains configuration")
}
```

3. Replace the entire status update section at the end of Reconcile() (after getting statefulSet) with:
```go
    // --- Begin status update section ---
    dittoServerCopy := dittoServer.DeepCopy()
    dittoServerCopy.Status.ObservedGeneration = dittoServer.Generation
    dittoServerCopy.Status.Replicas = replicas
    dittoServerCopy.Status.ReadyReplicas = statefulSet.Status.ReadyReplicas
    dittoServerCopy.Status.AvailableReplicas = statefulSet.Status.ReadyReplicas // StatefulSet uses ReadyReplicas
    dittoServerCopy.Status.ConfigHash = configHash

    // Set PerconaClusterName if Percona is enabled
    if dittoServer.Spec.Percona != nil && dittoServer.Spec.Percona.Enabled {
        dittoServerCopy.Status.PerconaClusterName = percona.ClusterName(dittoServer.Name)
    } else {
        dittoServerCopy.Status.PerconaClusterName = ""
    }

    // Set Phase
    if replicas == 0 {
        dittoServerCopy.Status.Phase = "Stopped"
    } else if statefulSet.Status.ReadyReplicas == replicas {
        dittoServerCopy.Status.Phase = "Running"
    } else {
        dittoServerCopy.Status.Phase = "Pending"
    }

    dittoServerCopy.Status.NFSEndpoint = fmt.Sprintf("%s-file.%s.svc.cluster.local:%d",
        dittoServer.Name, dittoServer.Namespace, nfs.GetNFSPort(dittoServer))

    // Set ConfigReady condition
    r.updateConfigReadyCondition(ctx, dittoServer, &dittoServerCopy.Status)

    // Set DatabaseReady condition (only relevant when Percona enabled)
    if dittoServer.Spec.Percona != nil && dittoServer.Spec.Percona.Enabled {
        pgCluster := &pgv2.PerconaPGCluster{}
        err := r.Get(ctx, client.ObjectKey{
            Namespace: dittoServer.Namespace,
            Name:      percona.ClusterName(dittoServer.Name),
        }, pgCluster)
        if err != nil {
            conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
                conditions.ConditionDatabaseReady, metav1.ConditionFalse, "PerconaNotFound",
                fmt.Sprintf("PerconaPGCluster not found: %v", err))
        } else if !percona.IsReady(pgCluster) {
            conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
                conditions.ConditionDatabaseReady, metav1.ConditionFalse, "PerconaNotReady",
                fmt.Sprintf("PostgreSQL cluster state: %s", percona.GetState(pgCluster)))
        } else {
            conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
                conditions.ConditionDatabaseReady, metav1.ConditionTrue, "PerconaReady",
                "PostgreSQL cluster is ready")
        }
    } else {
        // Remove DatabaseReady condition if Percona not enabled
        conditions.RemoveCondition(&dittoServerCopy.Status.Conditions, conditions.ConditionDatabaseReady)
    }

    // Set Available condition
    if replicas == 0 {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionAvailable, metav1.ConditionTrue, "Stopped",
            "DittoServer is stopped (replicas=0)")
    } else if statefulSet.Status.ReadyReplicas >= 1 {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionAvailable, metav1.ConditionTrue, "MinimumReplicasAvailable",
            fmt.Sprintf("StatefulSet has %d/%d ready replicas", statefulSet.Status.ReadyReplicas, replicas))
    } else {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionAvailable, metav1.ConditionFalse, "NoReplicasAvailable",
            fmt.Sprintf("Waiting for replicas: %d/%d ready", statefulSet.Status.ReadyReplicas, replicas))
    }

    // Set Progressing condition
    if statefulSet.Status.ObservedGeneration < statefulSet.Generation {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionProgressing, metav1.ConditionTrue, "StatefulSetUpdating",
            "StatefulSet is being updated")
    } else if statefulSet.Status.ReadyReplicas != replicas {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionProgressing, metav1.ConditionTrue, "ScalingReplicas",
            fmt.Sprintf("Scaling: %d/%d replicas ready", statefulSet.Status.ReadyReplicas, replicas))
    } else {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionProgressing, metav1.ConditionFalse, "ReconcileComplete",
            "All resources are up to date")
    }

    // Set Ready condition (aggregate of other conditions)
    configReady := conditions.IsConditionTrue(dittoServerCopy.Status.Conditions, conditions.ConditionConfigReady)
    available := conditions.IsConditionTrue(dittoServerCopy.Status.Conditions, conditions.ConditionAvailable)
    notProgressing := !conditions.IsConditionTrue(dittoServerCopy.Status.Conditions, conditions.ConditionProgressing)

    // DatabaseReady only required if Percona enabled
    databaseReady := true
    if dittoServer.Spec.Percona != nil && dittoServer.Spec.Percona.Enabled {
        databaseReady = conditions.IsConditionTrue(dittoServerCopy.Status.Conditions, conditions.ConditionDatabaseReady)
    }

    if configReady && available && notProgressing && databaseReady {
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionReady, metav1.ConditionTrue, "AllConditionsMet",
            "DittoServer is fully operational")
    } else {
        var reasons []string
        if !configReady {
            reasons = append(reasons, "ConfigNotReady")
        }
        if !available {
            reasons = append(reasons, "NotAvailable")
        }
        if !notProgressing {
            reasons = append(reasons, "StillProgressing")
        }
        if !databaseReady {
            reasons = append(reasons, "DatabaseNotReady")
        }
        conditions.SetCondition(&dittoServerCopy.Status.Conditions, dittoServer.Generation,
            conditions.ConditionReady, metav1.ConditionFalse, "ConditionsNotMet",
            fmt.Sprintf("Not ready: %v", reasons))
    }

    if err := r.Status().Update(ctx, dittoServerCopy); err != nil {
        logger.Error(err, "Failed to update DittoServer status")
        return ctrl.Result{}, err
    }

    return ctrl.Result{}, nil
    // --- End status update section ---
```

4. Move the configHash computation to be available for status (it's already computed earlier in reconcileStatefulSet but not available in Reconcile). Add this variable declaration at the start of Reconcile, after getting dittoServer:
```go
    var configHash string
```

Then modify reconcileStatefulSet to return the configHash:
- Change signature to: `func (r *DittoServerReconciler) reconcileStatefulSet(ctx context.Context, dittoServer *dittoiov1alpha1.DittoServer, replicas int32) (string, error)`
- At the end of the function before `return err`, return `configHash, err` (after CreateOrUpdate)
- Update the call site: `configHash, err = r.reconcileStatefulSet(ctx, dittoServer, replicas)`
  </action>
  <verify>
Run from k8s/dittofs-operator:
```bash
go build ./...
go test ./... -v
```
Build and tests pass.
  </verify>
  <done>
- Reconciler sets all five conditions: Ready, Available, ConfigReady, DatabaseReady (when Percona enabled), Progressing
- Ready is aggregate: true only when ConfigReady AND Available AND NOT Progressing AND (DatabaseReady if Percona enabled)
- Status includes: ObservedGeneration, Replicas, ReadyReplicas, AvailableReplicas, ConfigHash, PerconaClusterName
- DatabaseReady condition removed when Percona not enabled
  </done>
</task>

<task type="auto">
  <name>Task 3: Regenerate CRD manifests and verify</name>
  <files>
    k8s/dittofs-operator/config/crd/bases/dittofs.dittofs.com_dittoservers.yaml
  </files>
  <action>
1. Run make generate and manifests from the operator directory:
```bash
cd k8s/dittofs-operator && make generate manifests
```

2. Verify the generated CRD has the new status fields and print columns by checking the YAML output.

3. Run the full test suite to ensure nothing is broken:
```bash
go test ./... -v
```
  </action>
  <verify>
```bash
cd /Users/marmos91/Projects/dittofs/k8s/dittofs-operator && make generate manifests && go test ./... -v
```
Commands succeed, CRD YAML contains:
- `observedGeneration` in status schema
- `readyReplicas` in status schema
- `availableReplicas` in status schema
- `configHash` in status schema
- `perconaClusterName` in status schema
- Print columns show Ready, Available, Status
  </verify>
  <done>
- CRD manifest regenerated with new status fields
- Print columns updated: Replicas, Ready, Available, Status, Age
- All tests pass
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Build succeeds:
```bash
cd k8s/dittofs-operator && go build ./...
```

2. Tests pass:
```bash
cd k8s/dittofs-operator && go test ./... -v
```

3. CRD manifest has correct schema (spot check):
```bash
grep -A2 "observedGeneration" k8s/dittofs-operator/config/crd/bases/dittofs.dittofs.com_dittoservers.yaml
grep "printcolumn" k8s/dittofs-operator/api/v1alpha1/dittoserver_types.go
```
</verification>

<success_criteria>
- DittoServerStatus struct has: ObservedGeneration, Replicas, ReadyReplicas, AvailableReplicas, Phase, ConfigHash, PerconaClusterName, Conditions
- Condition type constants exist: ConditionReady, ConditionAvailable, ConditionConfigReady, ConditionDatabaseReady, ConditionProgressing
- Reconciler sets all five conditions based on actual resource state
- Ready condition is aggregate of other conditions
- kubectl print columns: Replicas, Ready, Available, Status, Age
- All tests pass, build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/05-status-conditions-and-lifecycle/05-01-SUMMARY.md`
</output>
