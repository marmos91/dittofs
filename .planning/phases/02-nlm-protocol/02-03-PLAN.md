---
phase: 02-nlm-protocol
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - internal/protocol/nlm/blocking/queue.go
  - internal/protocol/nlm/blocking/waiter.go
  - internal/protocol/nlm/callback/client.go
  - internal/protocol/nlm/callback/granted.go
  - internal/protocol/nlm/handlers/lock.go
  - internal/protocol/nlm/handlers/cancel.go
  - internal/protocol/nlm/handlers/granted.go
  - internal/protocol/nlm/handlers/handler.go
  - internal/protocol/nlm/metrics.go
  - pkg/metadata/service.go
  - pkg/adapter/nfs/nfs_adapter.go
autonomous: true

must_haves:
  truths:
    - "Blocking lock requests return NLM4_BLOCKED and queue the waiter"
    - "When lock is released, queued waiters are processed in FIFO order"
    - "NLM_GRANTED callback sent to client when blocked lock becomes available"
    - "Callback failure releases the lock immediately (no orphaned grants)"
    - "NLM_CANCEL removes waiter from queue and returns NLM4_GRANTED"
    - "Queue full returns NLM4_DENIED_NOLOCKS"
    - "Prometheus metrics track NLM operations"
  artifacts:
    - path: "internal/protocol/nlm/blocking/queue.go"
      provides: "Per-file blocking lock queue"
      exports: ["BlockingQueue", "NewBlockingQueue"]
    - path: "internal/protocol/nlm/blocking/waiter.go"
      provides: "Waiter entry with callback info"
      exports: ["Waiter"]
    - path: "internal/protocol/nlm/callback/client.go"
      provides: "TCP callback client for NLM_GRANTED"
      exports: ["SendGrantedCallback"]
    - path: "internal/protocol/nlm/metrics.go"
      provides: "NLM Prometheus metrics"
      exports: ["Metrics", "NewMetrics"]
  key_links:
    - from: "internal/protocol/nlm/handlers/lock.go"
      to: "internal/protocol/nlm/blocking"
      via: "queue waiter on conflict"
      pattern: "blockingQueue.*Enqueue"
    - from: "internal/protocol/nlm/callback/granted.go"
      to: "internal/protocol/nlm/callback/client.go"
      via: "send callback"
      pattern: "SendGrantedCallback"
    - from: "internal/protocol/nlm/handlers/handler.go"
      to: "internal/protocol/nlm/blocking"
      via: "handler stores blockingQueue reference"
      pattern: "blockingQueue.*BlockingQueue"
    - from: "pkg/metadata/service.go"
      to: "unlock callback function"
      via: "OnUnlockCallback invoked after lock release"
      pattern: "OnUnlockCallback"
---

<objective>
Implement blocking lock queue and NLM_GRANTED callback mechanism.

Purpose: Enable NFSv3 clients to use blocking (waiting) locks. When a lock request conflicts, the client waits and receives an NLM_GRANTED callback when the lock becomes available.

Output:
- Per-file blocking lock queue with configurable size limit
- NLM_GRANTED callback client with 5-second total timeout (per CONTEXT.md decision)
- Integration with unlock path to process waiters
- NLM-specific Prometheus metrics
</objective>

<execution_context>
@/Users/marmos91/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marmos91/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-nlm-protocol/02-CONTEXT.md
@.planning/phases/02-nlm-protocol/02-RESEARCH.md
@.planning/phases/02-nlm-protocol/02-01-SUMMARY.md
@.planning/phases/02-nlm-protocol/02-02-SUMMARY.md
@internal/protocol/nlm/handlers/lock.go
@internal/protocol/nlm/handlers/cancel.go
@internal/protocol/nlm/handlers/handler.go
@pkg/metadata/service.go
@pkg/adapter/nfs/nfs_adapter.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create blocking lock queue infrastructure</name>
  <files>
    internal/protocol/nlm/blocking/waiter.go
    internal/protocol/nlm/blocking/queue.go
  </files>
  <action>
Create the blocking lock queue infrastructure per CONTEXT.md decisions:

1. Create internal/protocol/nlm/blocking/waiter.go:
```go
package blocking

import (
    "sync"
    "time"

    "github.com/marmos91/dittofs/pkg/metadata/lock"
)

// Waiter represents a pending blocking lock request.
type Waiter struct {
    // Lock is the requested lock
    Lock *lock.EnhancedLock

    // Cookie is the client's opaque cookie (echoed in callback)
    Cookie []byte

    // Exclusive is whether the lock is exclusive
    Exclusive bool

    // CallbackAddr is the client's callback address (IP:port)
    CallbackAddr string

    // CallbackProg is the client's callback program number (NLM)
    CallbackProg uint32

    // CallbackVers is the callback program version
    CallbackVers uint32

    // CallerName is the client hostname from the lock request
    CallerName string

    // Svid is the client's process ID
    Svid int32

    // OH is the client's owner handle (opaque)
    OH []byte

    // FileHandle is the file this lock is for
    FileHandle []byte

    // QueuedAt is when this waiter was queued
    QueuedAt time.Time

    // Cancelled indicates if this waiter has been cancelled
    Cancelled bool

    // mu protects the Cancelled field
    mu sync.Mutex
}

// IsCancelled returns true if this waiter has been cancelled.
func (w *Waiter) IsCancelled() bool {
    w.mu.Lock()
    defer w.mu.Unlock()
    return w.Cancelled
}

// Cancel marks this waiter as cancelled.
func (w *Waiter) Cancel() {
    w.mu.Lock()
    defer w.mu.Unlock()
    w.Cancelled = true
}
```

2. Create internal/protocol/nlm/blocking/queue.go:
```go
package blocking

import (
    "sync"

    "github.com/marmos91/dittofs/pkg/metadata/lock"
)

// ErrQueueFull is returned when the per-file queue is at capacity.
var ErrQueueFull = errors.New("blocking queue full")

// BlockingQueue manages per-file queues of waiting lock requests.
type BlockingQueue struct {
    mu       sync.RWMutex
    queues   map[string][]*Waiter // fileHandle -> waiters (slice, not channel)
    maxQueue int                  // Per-file limit (e.g., 100)
}

// NewBlockingQueue creates a new blocking queue with the given per-file limit.
func NewBlockingQueue(maxPerFile int) *BlockingQueue {
    return &BlockingQueue{
        queues:   make(map[string][]*Waiter),
        maxQueue: maxPerFile,
    }
}

// Enqueue adds a waiter to the queue for a file.
// Returns ErrQueueFull if the per-file limit is reached.
func (bq *BlockingQueue) Enqueue(fileHandle string, waiter *Waiter) error {
    bq.mu.Lock()
    defer bq.mu.Unlock()

    queue := bq.queues[fileHandle]
    if len(queue) >= bq.maxQueue {
        return ErrQueueFull
    }

    waiter.QueuedAt = time.Now()
    bq.queues[fileHandle] = append(queue, waiter)
    return nil
}

// Cancel removes a waiter matching the given owner and range.
// Returns true if a waiter was found and cancelled.
func (bq *BlockingQueue) Cancel(fileHandle string, ownerID string, offset, length uint64) bool {
    bq.mu.Lock()
    defer bq.mu.Unlock()

    queue := bq.queues[fileHandle]
    for i, w := range queue {
        if w.Lock.Owner.OwnerID == ownerID &&
           w.Lock.Offset == offset &&
           w.Lock.Length == length {
            // Mark as cancelled
            w.Cancel()
            // Remove from queue
            bq.queues[fileHandle] = append(queue[:i], queue[i+1:]...)
            if len(bq.queues[fileHandle]) == 0 {
                delete(bq.queues, fileHandle)
            }
            return true
        }
    }
    return false
}

// GetWaiters returns a copy of all waiters for a file.
// Used by ProcessRelease to try granting locks.
func (bq *BlockingQueue) GetWaiters(fileHandle string) []*Waiter {
    bq.mu.RLock()
    defer bq.mu.RUnlock()

    queue := bq.queues[fileHandle]
    if len(queue) == 0 {
        return nil
    }

    // Return a copy
    result := make([]*Waiter, len(queue))
    copy(result, queue)
    return result
}

// RemoveWaiter removes a specific waiter from the queue.
// Called after successfully granting a lock.
func (bq *BlockingQueue) RemoveWaiter(fileHandle string, waiter *Waiter) {
    bq.mu.Lock()
    defer bq.mu.Unlock()

    queue := bq.queues[fileHandle]
    for i, w := range queue {
        if w == waiter {
            bq.queues[fileHandle] = append(queue[:i], queue[i+1:]...)
            if len(bq.queues[fileHandle]) == 0 {
                delete(bq.queues, fileHandle)
            }
            return
        }
    }
}

// TotalWaiters returns the total number of waiters across all files.
// Used for metrics.
func (bq *BlockingQueue) TotalWaiters() int {
    bq.mu.RLock()
    defer bq.mu.RUnlock()

    total := 0
    for _, queue := range bq.queues {
        total += len(queue)
    }
    return total
}
```
  </action>
  <verify>
Run: `go build ./internal/protocol/nlm/blocking/...` - compiles without errors
Run: `go vet ./internal/protocol/nlm/blocking/...` - no issues
  </verify>
  <done>
Blocking queue infrastructure exists with Waiter type and BlockingQueue with per-file limits.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement NLM_GRANTED callback client with 5s total timeout</name>
  <files>
    internal/protocol/nlm/callback/client.go
    internal/protocol/nlm/callback/granted.go
    internal/protocol/nlm/handlers/granted.go
  </files>
  <action>
Create the callback client for NLM_GRANTED notifications per CONTEXT.md locked decision:
**5 second timeout for NLM_GRANTED callbacks (total, not per-operation)**

1. Create internal/protocol/nlm/callback/client.go:
```go
package callback

import (
    "bytes"
    "context"
    "fmt"
    "net"
    "time"

    "github.com/marmos91/dittofs/internal/protocol/nfs/rpc"
    "github.com/marmos91/dittofs/internal/protocol/nlm"
    nlm_xdr "github.com/marmos91/dittofs/internal/protocol/nlm/xdr"
)

const (
    // CallbackTimeout is the TOTAL timeout for NLM_GRANTED callbacks per CONTEXT.md decision.
    // This is a locked decision: 5 second total timeout (not per-operation).
    // The deadline applies to both dial and I/O combined.
    CallbackTimeout = 5 * time.Second
)

// SendGrantedCallback sends an NLM_GRANTED callback to a client.
// Returns nil on success, error if callback failed.
// Per CONTEXT.md: fresh TCP connection for each callback, 5s TOTAL timeout.
func SendGrantedCallback(
    ctx context.Context,
    addr string,
    prog uint32,
    vers uint32,
    args *nlm.NLM4GrantedArgs,
) error {
    // Create a context with 5s total deadline for the entire operation
    // This is a LOCKED DECISION from CONTEXT.md: 5 second timeout total
    callbackCtx, cancel := context.WithTimeout(ctx, CallbackTimeout)
    defer cancel()

    // Create TCP connection using the context deadline for dial
    // The DialContext respects the context deadline, so dial will be bounded by the remaining time
    var dialer net.Dialer
    conn, err := dialer.DialContext(callbackCtx, "tcp", addr)
    if err != nil {
        return fmt.Errorf("dial callback address %s: %w", addr, err)
    }
    defer conn.Close()

    // Get the absolute deadline from context and set on connection for I/O
    // This ensures remaining time after dial is used for I/O
    if deadline, ok := callbackCtx.Deadline(); ok {
        if err := conn.SetDeadline(deadline); err != nil {
            return fmt.Errorf("set deadline: %w", err)
        }
    }

    // Encode NLM_GRANTED args
    var argsBuf bytes.Buffer
    if err := nlm_xdr.EncodeNLM4GrantedArgs(&argsBuf, args); err != nil {
        return fmt.Errorf("encode granted args: %w", err)
    }

    // Build RPC call message
    // XID can be any unique value - use current time nanoseconds
    xid := uint32(time.Now().UnixNano() & 0xFFFFFFFF)

    callMsg, err := rpc.MakeCallMessage(xid, prog, vers, nlm.NLMProcGranted, argsBuf.Bytes())
    if err != nil {
        return fmt.Errorf("make call message: %w", err)
    }

    // Send the call
    if _, err := conn.Write(callMsg); err != nil {
        return fmt.Errorf("write call: %w", err)
    }

    // Read and validate response (we don't care about the result per CONTEXT.md,
    // but we need to wait for it to confirm delivery)
    // Read fragment header (4 bytes)
    var headerBuf [4]byte
    if _, err := conn.Read(headerBuf[:]); err != nil {
        return fmt.Errorf("read reply header: %w", err)
    }

    // Parse fragment length
    header := uint32(headerBuf[0])<<24 | uint32(headerBuf[1])<<16 |
              uint32(headerBuf[2])<<8 | uint32(headerBuf[3])
    fragLen := header & 0x7FFFFFFF

    // Read reply body (just discard - we don't care about status per CONTEXT.md)
    replyBuf := make([]byte, fragLen)
    if _, err := conn.Read(replyBuf); err != nil {
        return fmt.Errorf("read reply body: %w", err)
    }

    return nil
}
```

2. Create internal/protocol/nlm/callback/granted.go:
```go
package callback

import (
    "context"

    "github.com/marmos91/dittofs/internal/logger"
    "github.com/marmos91/dittofs/internal/protocol/nlm"
    "github.com/marmos91/dittofs/internal/protocol/nlm/blocking"
    "github.com/marmos91/dittofs/pkg/metadata/lock"
)

// ProcessGrantedCallback sends NLM_GRANTED callback to a waiter.
// If callback fails, releases the lock immediately per CONTEXT.md locked decision.
// Returns true if callback succeeded, false if it failed.
func ProcessGrantedCallback(
    ctx context.Context,
    waiter *blocking.Waiter,
    lm *lock.Manager,
) bool {
    // Check if cancelled while we were processing
    if waiter.IsCancelled() {
        logger.Debug("Skipping callback for cancelled waiter",
            "owner", waiter.Lock.Owner.OwnerID)
        return false
    }

    // Build NLM_GRANTED args
    args := &nlm.NLM4GrantedArgs{
        Cookie:    waiter.Cookie,
        Exclusive: waiter.Exclusive,
        Lock: nlm.NLM4Lock{
            CallerName: waiter.CallerName,
            FH:         waiter.FileHandle,
            OH:         waiter.OH,
            Svid:       waiter.Svid,
            Offset:     waiter.Lock.Offset,
            Length:     waiter.Lock.Length,
        },
    }

    // Send callback
    err := SendGrantedCallback(ctx, waiter.CallbackAddr, waiter.CallbackProg,
                               waiter.CallbackVers, args)
    if err != nil {
        logger.Warn("NLM_GRANTED callback failed, releasing lock",
            "error", err,
            "addr", waiter.CallbackAddr,
            "owner", waiter.Lock.Owner.OwnerID)

        // Per CONTEXT.md locked decision: release lock immediately if callback fails
        handleKey := string(waiter.Lock.FileHandle)
        _ = lm.RemoveEnhancedLock(handleKey, waiter.Lock.Owner,
                                  waiter.Lock.Offset, waiter.Lock.Length)
        return false
    }

    logger.Debug("NLM_GRANTED callback succeeded",
        "addr", waiter.CallbackAddr,
        "owner", waiter.Lock.Owner.OwnerID)
    return true
}
```

3. Create internal/protocol/nlm/handlers/granted.go:
   - Handler for NLM_GRANTED procedure (receives responses to our callbacks)
   - Most NLM servers ignore the response, but we need to handle it
   - Simply return NLM4_GRANTED (acknowledge receipt)
  </action>
  <verify>
Run: `go build ./internal/protocol/nlm/callback/...` - compiles without errors
Run: `go build ./internal/protocol/nlm/handlers/...` - compiles with granted.go
  </verify>
  <done>
NLM_GRANTED callback client implemented with 5-second TOTAL timeout (per CONTEXT.md locked decision) and lock release on failure.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate blocking queue with lock/unlock handlers</name>
  <files>
    internal/protocol/nlm/handlers/handler.go
    internal/protocol/nlm/handlers/lock.go
    internal/protocol/nlm/handlers/cancel.go
    pkg/metadata/service.go
    pkg/adapter/nfs/nfs_adapter.go
  </files>
  <action>
Wire the blocking queue into the NLM handlers and unlock path:

1. Update internal/protocol/nlm/handlers/handler.go:
   - Add blockingQueue *blocking.BlockingQueue field to Handler struct
   - Update NewHandler to accept and store blocking queue:
     ```go
     func NewHandler(metadataService *metadata.MetadataService, blockingQueue *blocking.BlockingQueue) *Handler {
         return &Handler{
             metadataService: metadataService,
             blockingQueue:   blockingQueue,
         }
     }
     ```
   - Add getter: GetBlockingQueue() *blocking.BlockingQueue

2. Update internal/protocol/nlm/handlers/lock.go to use blocking queue:
   - If block=true and lock conflicts:
     a. Create Waiter with callback info from request
     b. Extract callback address from client (use connection remote addr + callback port from request if provided, else use standard NLM port)
     c. Enqueue waiter via handler.blockingQueue.Enqueue()
     d. If queue full (ErrQueueFull): return NLM4_DENIED_NOLOCKS
     e. Else: return NLM4_BLOCKED (client will wait for callback)

3. Update internal/protocol/nlm/handlers/cancel.go:
   - Call handler.blockingQueue.Cancel() to find and remove the waiter
   - Return NLM4_GRANTED (always succeeds per CONTEXT.md)

4. Update pkg/metadata/service.go to support unlock callback:
   - Add OnUnlockCallback field to MetadataService or accept callback in UnlockFileNLM
   - Option A (cleaner): Add SetUnlockCallback(fn func(handle FileHandle)) method
   - Option B: Pass callback to UnlockFileNLM as parameter

   Use Option A - add to MetadataService:
   ```go
   type MetadataService struct {
       // ... existing fields
       onUnlockCallback func(handle FileHandle)
   }

   // SetNLMUnlockCallback sets a callback invoked after each unlock.
   // The NLM blocking queue uses this to process waiting locks.
   func (s *MetadataService) SetNLMUnlockCallback(fn func(handle FileHandle)) {
       s.onUnlockCallback = fn
   }
   ```

   Update UnlockFileNLM to invoke callback after successful unlock:
   ```go
   func (s *MetadataService) UnlockFileNLM(...) error {
       // ... existing unlock logic ...

       // Notify NLM blocking queue that a lock was released
       if s.onUnlockCallback != nil {
           s.onUnlockCallback(handle)
       }
       return nil
   }
   ```

5. Update pkg/adapter/nfs/nfs_adapter.go:
   - Create BlockingQueue in SetRuntime (or NewNFSAdapter)
   - Pass to NLM handler: nlm_handlers.NewHandler(metadataService, blockingQueue)
   - Set unlock callback on MetadataService:
     ```go
     metadataService.SetNLMUnlockCallback(func(handle metadata.FileHandle) {
         // Process waiters for this file handle
         go a.processNLMWaiters(handle)
     })
     ```
   - Implement processNLMWaiters(handle):
     a. Get waiters via blockingQueue.GetWaiters(string(handle))
     b. For each waiter in FIFO order:
        - Skip if cancelled
        - Try to acquire the lock via metadataService.LockFileNLM()
        - If success: send callback, remove from queue
        - If conflict: skip (try next waiter)
        - If callback fails: lock already released by ProcessGrantedCallback
     c. Use goroutine for each callback to avoid blocking unlock path
  </action>
  <verify>
Run: `go build ./internal/protocol/nlm/handlers/...` - compiles
Run: `go build ./pkg/metadata/...` - compiles
Run: `go build ./pkg/adapter/nfs/...` - compiles
Run: `go test ./...` - all tests pass
  </verify>
  <done>
Blocking queue integrated with handlers. Lock handler queues waiters on conflict. Cancel handler removes waiters. Unlock triggers waiter processing via callback.
  </done>
</task>

<task type="auto">
  <name>Task 4: Add NLM Prometheus metrics</name>
  <files>
    internal/protocol/nlm/metrics.go
  </files>
  <action>
Create NLM-specific Prometheus metrics:

Create internal/protocol/nlm/metrics.go:
```go
package nlm

import (
    "github.com/prometheus/client_golang/prometheus"
)

// Metrics tracks NLM-specific metrics.
type Metrics struct {
    // RequestsTotal counts NLM requests by procedure and status
    RequestsTotal *prometheus.CounterVec

    // RequestDuration tracks latency distribution
    RequestDuration *prometheus.HistogramVec

    // BlockingQueueSize tracks current queue depth per file
    BlockingQueueSize prometheus.Gauge

    // CallbacksTotal counts NLM_GRANTED callbacks by result
    CallbacksTotal *prometheus.CounterVec

    // CallbackDuration tracks callback latency
    CallbackDuration prometheus.Histogram
}

// NewMetrics creates NLM metrics with nlm_ prefix.
func NewMetrics(reg prometheus.Registerer) *Metrics {
    m := &Metrics{
        RequestsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "nlm_requests_total",
                Help: "Total NLM requests by procedure and status",
            },
            []string{"procedure", "status"},
        ),
        RequestDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "nlm_request_duration_seconds",
                Help:    "NLM request duration in seconds",
                Buckets: prometheus.DefBuckets,
            },
            []string{"procedure"},
        ),
        BlockingQueueSize: prometheus.NewGauge(
            prometheus.GaugeOpts{
                Name: "nlm_blocking_queue_size",
                Help: "Current number of waiting lock requests",
            },
        ),
        CallbacksTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "nlm_callbacks_total",
                Help: "Total NLM_GRANTED callbacks by result",
            },
            []string{"result"}, // "success", "failed", "cancelled"
        ),
        CallbackDuration: prometheus.NewHistogram(
            prometheus.HistogramOpts{
                Name:    "nlm_callback_duration_seconds",
                Help:    "NLM_GRANTED callback duration in seconds",
                Buckets: prometheus.DefBuckets,
            },
        ),
    }

    // Register all metrics
    reg.MustRegister(m.RequestsTotal, m.RequestDuration,
                     m.BlockingQueueSize, m.CallbacksTotal, m.CallbackDuration)

    return m
}
```

Wire metrics into handlers:
- In dispatch.go: record RequestsTotal and RequestDuration on each request completion
- In adapter: periodically update BlockingQueueSize from blockingQueue.TotalWaiters()
- In callback/granted.go: record CallbacksTotal and CallbackDuration around SendGrantedCallback
  </action>
  <verify>
Run: `go build ./internal/protocol/nlm/...` - compiles
Run: `grep "nlm_requests_total" internal/protocol/nlm/metrics.go` - shows metrics
  </verify>
  <done>
NLM Prometheus metrics registered with nlm_* prefix. Metrics wired into handlers and callback client.
  </done>
</task>

</tasks>

<verification>
Overall verification for Plan 02-03:

1. Blocking queue infrastructure:
   - `ls internal/protocol/nlm/blocking/` shows queue.go, waiter.go
   - `go build ./internal/protocol/nlm/blocking/...` compiles

2. Callback client:
   - `ls internal/protocol/nlm/callback/` shows client.go, granted.go
   - `go build ./internal/protocol/nlm/callback/...` compiles
   - `grep "CallbackTimeout = 5" internal/protocol/nlm/callback/client.go` confirms 5s total timeout

3. Integration:
   - `grep "blockingQueue" internal/protocol/nlm/handlers/handler.go` shows queue field
   - `grep "blockingQueue" internal/protocol/nlm/handlers/lock.go` shows queue usage
   - `grep "SetNLMUnlockCallback" pkg/metadata/service.go` shows callback method
   - `grep "onUnlockCallback" pkg/metadata/service.go` shows callback field

4. Metrics:
   - `grep "nlm_requests_total" internal/protocol/nlm/metrics.go` shows metrics

5. Full test suite:
   - `go test ./...` passes
</verification>

<success_criteria>
- Blocking lock requests (block=true) return NLM4_BLOCKED and queue waiter
- Queue full returns NLM4_DENIED_NOLOCKS
- Lock release processes queued waiters in FIFO order
- NLM_GRANTED callback sent with 5-second TOTAL timeout (per CONTEXT.md locked decision)
- Callback failure releases the lock immediately
- NLM_CANCEL removes waiter from queue
- MetadataService invokes unlock callback which triggers waiter processing
- Prometheus metrics track NLM operations with nlm_* prefix
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-nlm-protocol/02-03-SUMMARY.md`
</output>
