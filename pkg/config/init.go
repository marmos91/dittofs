package config

import (
	"fmt"
	"os"
	"path/filepath"
)

// InitConfig creates a sample configuration file at the default location.
//
// This function:
//  1. Creates the config directory if it doesn't exist
//  2. Generates a config file with defaults and helpful comments
//  3. Returns an error if the file already exists (won't overwrite)
//
// Parameters:
//   - force: If true, overwrite existing config file
//
// Returns:
//   - string: Path to the created config file
//   - error: File creation error or file already exists
func InitConfig(force bool) (string, error) {
	configPath := GetDefaultConfigPath()
	configDir := filepath.Dir(configPath)

	// Create config directory if it doesn't exist
	if err := os.MkdirAll(configDir, 0755); err != nil {
		return "", fmt.Errorf("failed to create config directory: %w", err)
	}

	// Check if config already exists
	if !force && DefaultConfigExists() {
		return "", fmt.Errorf("config file already exists at %s (use --force to overwrite)", configPath)
	}

	// Generate sample config
	cfg := GetDefaultConfig()

	// Marshal to YAML with comments
	yamlData, err := generateYAMLWithComments(cfg)
	if err != nil {
		return "", fmt.Errorf("failed to generate YAML: %w", err)
	}

	// Write to file
	if err := os.WriteFile(configPath, []byte(yamlData), 0644); err != nil {
		return "", fmt.Errorf("failed to write config file: %w", err)
	}

	return configPath, nil
}

// generateYAMLWithComments generates a YAML config file with helpful comments.
func generateYAMLWithComments(cfg *Config) (string, error) {
	// We'll build the YAML manually with comments for better UX
	// This is more maintainable than trying to preserve comments through yaml.Marshal

	// Get first store names for the example
	var metadataStoreName string
	var metadataStore MetadataStoreConfig
	var payloadStoreName string
	var payloadStore PayloadStoreConfig

	for name, store := range cfg.Metadata.Stores {
		metadataStoreName = name
		metadataStore = store
		break
	}
	for name, store := range cfg.Payload.Stores {
		payloadStoreName = name
		payloadStore = store
		break
	}

	yaml := `# DittoFS Configuration File
# This file is automatically generated with default values.
# Modify as needed for your environment.

# Logging configuration
logging:
  # Log level: DEBUG, INFO, WARN, ERROR
  level: "` + cfg.Logging.Level + `"
  # Log format: text, json
  format: "` + cfg.Logging.Format + `"
  # Log output: stdout, stderr, or file path
  output: "` + cfg.Logging.Output + `"

# OpenTelemetry distributed tracing configuration
# Traces are exported to an OTLP-compatible collector (Jaeger, Tempo, etc.)
telemetry:
  # Enable distributed tracing (default: false)
  enabled: ` + fmt.Sprintf("%t", cfg.Telemetry.Enabled) + `
  # OTLP collector endpoint (host:port)
  endpoint: "` + cfg.Telemetry.Endpoint + `"
  # Use insecure (non-TLS) connection (default: false)
  # Set to true for local development without TLS
  insecure: ` + fmt.Sprintf("%t", cfg.Telemetry.Insecure) + `
  # Trace sampling rate (0.0 to 1.0)
  # 1.0 = sample all, 0.5 = sample 50%
  sample_rate: ` + fmt.Sprintf("%.1f", cfg.Telemetry.SampleRate) + `

# Server-wide settings
server:
  # Maximum time to wait for graceful shutdown
  shutdown_timeout: ` + cfg.Server.ShutdownTimeout.String() + `

# Cache configuration (WAL-backed, mandatory for crash recovery)
# All writes go through the WAL cache for durability
cache:
  # Directory path for the cache WAL file (required)
  path: "` + cfg.Cache.Path + `"
  # Maximum cache size (supports human-readable formats: "1GB", "512MB", "10Gi")
  size: 1Gi

# Payload store configuration (block storage for persistent data)
# Stores persist cache data to durable storage (S3, filesystem)
payload:
  # Named payload store instances
  # Shares reference these by name
  stores:
    ` + payloadStoreName + `:
      # Payload store type: s3, filesystem, memory
      type: "` + payloadStore.Type + `"`

	// Add type-specific configuration for payload store
	switch payloadStore.Type {
	case "filesystem":
		if payloadStore.Filesystem != nil {
			yaml += `
      # Filesystem configuration
      filesystem:
        # Directory path for block storage
        base_path: "` + payloadStore.Filesystem.BasePath + `"`
		}
	case "s3":
		if payloadStore.S3 != nil {
			yaml += `
      # S3 configuration
      s3:
        # S3 bucket name (required)
        bucket: "` + payloadStore.S3.Bucket + `"
        # AWS region (optional, uses SDK default if empty)
        region: "` + payloadStore.S3.Region + `"
        # S3 endpoint URL (optional, for S3-compatible services like MinIO)
        # endpoint: "http://localhost:9000"
        # Access credentials (optional, uses AWS SDK default chain if empty)
        # access_key_id: ""
        # secret_access_key: ""
        # Key prefix for all blocks
        prefix: "` + payloadStore.S3.Prefix + `"
        # Force path-style addressing (required for MinIO/Localstack)
        force_path_style: ` + fmt.Sprintf("%t", payloadStore.S3.ForcePathStyle)
		}
	case "memory":
		yaml += `
      # Memory store has no specific configuration (ephemeral, for testing)`
	}

	yaml += `

  # Transfer manager configuration (uploads/downloads to block store)
  transfer:
    workers:
      # Number of parallel upload workers
      uploads: ` + fmt.Sprintf("%d", cfg.Payload.Transfer.Workers.Uploads) + `
      # Number of parallel download workers
      downloads: ` + fmt.Sprintf("%d", cfg.Payload.Transfer.Workers.Downloads) + `

# Metadata store configuration
metadata:
  # Filesystem capabilities and limits
  filesystem_capabilities:
    # Maximum size of a single read operation (bytes)
    max_read_size: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.MaxReadSize) + `
    # Preferred read size for optimal performance (bytes)
    preferred_read_size: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.PreferredReadSize) + `
    # Maximum size of a single write operation (bytes)
    max_write_size: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.MaxWriteSize) + `
    # Preferred write size for optimal performance (bytes)
    preferred_write_size: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.PreferredWriteSize) + `
    # Maximum file size supported (bytes)
    max_file_size: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.MaxFileSize) + `
    # Maximum filename length (characters)
    max_filename_len: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.MaxFilenameLen) + `
    # Maximum path length (characters)
    max_path_len: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.MaxPathLen) + `
    # Maximum number of hard links per file
    max_hard_link_count: ` + fmt.Sprintf("%d", cfg.Metadata.FilesystemCapabilities.MaxHardLinkCount) + `
    # Whether hard links are supported
    supports_hard_links: ` + fmt.Sprintf("%t", cfg.Metadata.FilesystemCapabilities.SupportsHardLinks) + `
    # Whether symbolic links are supported
    supports_symlinks: ` + fmt.Sprintf("%t", cfg.Metadata.FilesystemCapabilities.SupportsSymlinks) + `
    # Whether filenames are case-sensitive
    case_sensitive: ` + fmt.Sprintf("%t", cfg.Metadata.FilesystemCapabilities.CaseSensitive) + `
    # Whether filename case is preserved
    case_preserving: ` + fmt.Sprintf("%t", cfg.Metadata.FilesystemCapabilities.CasePreserving) + `

  # Named metadata store instances
  stores:
    ` + metadataStoreName + `:
      # Metadata store type: memory, badger
      type: "` + metadataStore.Type + `"`

	// Add type-specific configuration for metadata
	switch metadataStore.Type {
	case "badger":
		yaml += `
      # BadgerDB configuration
      badger:
        # Directory path for BadgerDB database
        db_path: "` + fmt.Sprintf("%v", metadataStore.Badger["db_path"]) + `"`
	case "memory":
		yaml += `
      # Memory store has no specific configuration
      memory: {}`
	}

	yaml += `

# Shares/Exports configuration
shares:
  - # Share path (must start with /)
    name: "` + cfg.Shares[0].Name + `"
    # Reference to metadata store by name
    metadata: "` + cfg.Shares[0].Metadata + `"
    # Reference to payload store by name (defaults to first if empty)
    payload: "` + cfg.Shares[0].Payload + `"
    # Make share read-only
    read_only: ` + fmt.Sprintf("%t", cfg.Shares[0].ReadOnly) + `
    # IP addresses or CIDR ranges allowed to access (empty = all)
    allowed_clients: []
    # IP addresses or CIDR ranges explicitly denied
    denied_clients: []
    # Require authentication
    require_auth: ` + fmt.Sprintf("%t", cfg.Shares[0].RequireAuth) + `
    # Allowed authentication methods: anonymous, unix
    allowed_auth_methods:
      - "anonymous"
      - "unix"
    # Identity mapping configuration
    identity_mapping:
      # Map all users to anonymous (all_squash)
      map_all_to_anonymous: ` + fmt.Sprintf("%t", cfg.Shares[0].IdentityMapping.MapAllToAnonymous) + `
      # Map root user to anonymous (root_squash)
      map_privileged_to_anonymous: ` + fmt.Sprintf("%t", cfg.Shares[0].IdentityMapping.MapPrivilegedToAnonymous) + `
      # UID for anonymous users
      anonymous_uid: ` + fmt.Sprintf("%d", cfg.Shares[0].IdentityMapping.AnonymousUID) + `
      # GID for anonymous users
      anonymous_gid: ` + fmt.Sprintf("%d", cfg.Shares[0].IdentityMapping.AnonymousGID) + `
    # Root directory attributes
    root_directory_attributes:
      # Unix permission mode (octal)
      mode: ` + fmt.Sprintf("0%o", cfg.Shares[0].RootDirectoryAttributes.Mode) + `
      # Owner user ID
      # Note: For POSIX compliance testing with anonymous access, set this to anonymous_uid (65534)
      uid: ` + fmt.Sprintf("%d", cfg.Shares[0].RootDirectoryAttributes.UID) + `
      # Owner group ID
      # Note: For POSIX compliance testing with anonymous access, set this to anonymous_gid (65534)
      gid: ` + fmt.Sprintf("%d", cfg.Shares[0].RootDirectoryAttributes.GID) + `
    # Restrict DUMP operations to allowed clients only
    dump_restricted: ` + fmt.Sprintf("%t", cfg.Shares[0].DumpRestricted) + `
    # IP addresses or CIDR ranges allowed to use DUMP (only used if dump_restricted=true)
    dump_allowed_clients: []

# Protocol adapters configuration
adapters:
  # NFS adapter configuration
  nfs:
    # Enable NFS adapter
    enabled: ` + fmt.Sprintf("%t", cfg.Adapters.NFS.Enabled) + `
    # TCP port to listen on
    port: ` + fmt.Sprintf("%d", cfg.Adapters.NFS.Port) + `
    # Maximum concurrent connections (0 = unlimited)
    max_connections: ` + fmt.Sprintf("%d", cfg.Adapters.NFS.MaxConnections) + `
    # Timeout configuration
    timeouts:
      # Maximum time to read a request
      read: ` + cfg.Adapters.NFS.Timeouts.Read.String() + `
      # Maximum time to write a response
      write: ` + cfg.Adapters.NFS.Timeouts.Write.String() + `
      # Maximum idle time between requests
      idle: ` + cfg.Adapters.NFS.Timeouts.Idle.String() + `
      # Graceful shutdown timeout
      shutdown: ` + cfg.Adapters.NFS.Timeouts.Shutdown.String() + `
    # Metrics logging interval (0 = disabled)
    metrics_log_interval: ` + cfg.Adapters.NFS.MetricsLogInterval.String() + `

  # SMB adapter configuration
  smb:
    # Enable SMB adapter
    enabled: ` + fmt.Sprintf("%t", cfg.Adapters.SMB.Enabled) + `
    # TCP port to listen on (standard SMB port)
    port: ` + fmt.Sprintf("%d", cfg.Adapters.SMB.Port) + `
    # Maximum concurrent connections (0 = unlimited)
    max_connections: ` + fmt.Sprintf("%d", cfg.Adapters.SMB.MaxConnections) + `
    # Timeout configuration
    timeouts:
      # Maximum time to read a request
      read: ` + cfg.Adapters.SMB.Timeouts.Read.String() + `
      # Maximum time to write a response
      write: ` + cfg.Adapters.SMB.Timeouts.Write.String() + `
      # Maximum idle time between requests
      idle: ` + cfg.Adapters.SMB.Timeouts.Idle.String() + `
      # Graceful shutdown timeout
      shutdown: ` + cfg.Adapters.SMB.Timeouts.Shutdown.String() + `
    # Metrics logging interval (0 = disabled)
    metrics_log_interval: ` + cfg.Adapters.SMB.MetricsLogInterval.String() + `
    # SMB credit configuration for flow control
    credits:
      # Credit grant strategy: adaptive, fixed
      strategy: "` + cfg.Adapters.SMB.Credits.Strategy + `"
      # Minimum credits granted per request
      min_grant: ` + fmt.Sprintf("%d", cfg.Adapters.SMB.Credits.MinGrant) + `
      # Maximum credits granted per request
      max_grant: ` + fmt.Sprintf("%d", cfg.Adapters.SMB.Credits.MaxGrant) + `
      # Initial credits granted on session setup
      initial_grant: ` + fmt.Sprintf("%d", cfg.Adapters.SMB.Credits.InitialGrant) + `
`

	return yaml, nil
}

// InitConfigToPath creates a sample configuration file at the specified path.
//
// Similar to InitConfig but allows specifying a custom path.
//
// Parameters:
//   - path: Full path to the config file to create
//   - force: If true, overwrite existing config file
//
// Returns:
//   - error: File creation error or file already exists
func InitConfigToPath(path string, force bool) error {
	configDir := filepath.Dir(path)

	// Create config directory if it doesn't exist
	if err := os.MkdirAll(configDir, 0755); err != nil {
		return fmt.Errorf("failed to create config directory: %w", err)
	}

	// Check if config already exists
	if !force {
		if _, err := os.Stat(path); err == nil {
			return fmt.Errorf("config file already exists at %s (use --force to overwrite)", path)
		}
	}

	// Generate sample config with comments
	cfg := GetDefaultConfig()

	// Generate YAML with helpful comments
	yamlData, err := generateYAMLWithComments(cfg)
	if err != nil {
		return fmt.Errorf("failed to generate config: %w", err)
	}

	// Write to file
	if err := os.WriteFile(path, []byte(yamlData), 0644); err != nil {
		return fmt.Errorf("failed to write config file: %w", err)
	}

	return nil
}
